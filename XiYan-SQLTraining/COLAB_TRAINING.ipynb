{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# XiYan-SQL Training on Google Colab\n",
        "\n",
        "This notebook provides a complete step-by-step guide to train the XiYan-SQL model on Google Colab.\n",
        "\n",
        "## Prerequisites\n",
        "- Upload your model files to Google Drive (e.g., `Qwen2.5-Coder-3B-Instruct` folder)\n",
        "- Upload your dataset files to Google Drive (raw data, processed data, or both)\n",
        "- Enable GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step1"
      },
      "source": [
        "## Step 1: Install Dependencies\n",
        "\n",
        "Install all required packages for XiYan-SQL training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_deps"
      },
      "outputs": [],
      "source": [
        "# Install system dependencies\n",
        "!apt-get update -qq\n",
        "!apt-get install -y -qq libaio-dev  # Required for DeepSpeed\n",
        "\n",
        "# Install Python packages\n",
        "!pip install -q accelerate>=1.12.0\n",
        "!pip install -q datasets>=3.0.0\n",
        "!pip install -q deepspeed>=0.18.4\n",
        "!pip install -q llama-index>=0.9.6.post2\n",
        "!pip install -q markupsafe==2.1.3  # Pin to <3.0\n",
        "!pip install -q modelscope>=1.33.0\n",
        "!pip install -q mysql-connector-python>=9.5.0\n",
        "!pip install -q ninja>=1.13.0\n",
        "!pip install -q \"numpy>=1.23.0,<2.0\"\n",
        "!pip install -q packaging>=24.1\n",
        "!pip install -q pandas>=2.3.3\n",
        "!pip install -q peft==0.11.1\n",
        "!pip install -q \"protobuf>=6.33.3\"\n",
        "!pip install -q psycopg2-binary>=2.9.11\n",
        "!pip install -q sentencepiece>=0.2.1\n",
        "!pip install -q setuptools>=70.2.0\n",
        "!pip install -q sqlalchemy>=2.0.45\n",
        "!pip install -q sqlglot>=28.5.0\n",
        "!pip install -q swanlab>=0.7.6\n",
        "!pip install -q textdistance>=4.6.3\n",
        "!pip install -q \"torch==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install -q \"torchaudio==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install -q \"torchvision==0.24.0\" --index-url https://download.pytorch.org/whl/cu126\n",
        "!pip install -q transformers==4.42.3\n",
        "!pip install -q wheel>=0.45.1\n",
        "\n",
        "# Install flash-attn (optional, for faster attention)\n",
        "# Note: This may take a while to compile\n",
        "try:\n",
        "    !pip install -q flash-attn --no-build-isolation\n",
        "    print(\"‚úÖ flash-attn installed successfully\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  flash-attn installation failed, continuing without it\")\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step2"
      },
      "source": [
        "## Step 2: Clone Repository\n",
        "\n",
        "Clone the XiYan-SQL repository to Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clone_repo"
      },
      "outputs": [],
      "source": [
        "# Change to content directory\n",
        "import os\n",
        "import sys\n",
        "os.chdir('/content')\n",
        "\n",
        "# Clone the repository\n",
        "# Replace with your repository URL\n",
        "REPO_URL = \"https://github.com/rezaarrazi/XiYan-SQL.git\"  # ‚ö†Ô∏è UPDATE THIS\n",
        "\n",
        "if not os.path.exists('XiYan-SQL'):\n",
        "    os.system(f'git clone {REPO_URL}')\n",
        "    print(\"‚úÖ Repository cloned successfully\")\n",
        "else:\n",
        "    print(\"‚úÖ Repository already exists\")\n",
        "\n",
        "# Navigate to training directory\n",
        "os.chdir('XiYan-SQL/XiYan-SQLTraining')\n",
        "\n",
        "# Add to Python path so imports work correctly\n",
        "TRAINING_DIR = os.getcwd()\n",
        "if TRAINING_DIR not in sys.path:\n",
        "    sys.path.insert(0, TRAINING_DIR)\n",
        "if os.path.dirname(TRAINING_DIR) not in sys.path:\n",
        "    sys.path.insert(0, os.path.dirname(TRAINING_DIR))\n",
        "\n",
        "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")\n",
        "print(f\"‚úÖ Python path configured\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step3"
      },
      "source": [
        "## Step 3: Mount Google Drive\n",
        "\n",
        "Mount your Google Drive to access model and dataset files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount_drive"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "print(\"‚úÖ Google Drive mounted successfully\")\n",
        "print(\"\\nüìÇ Drive path: /content/drive/MyDrive\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step4"
      },
      "source": [
        "## Step 4: Copy Model from Google Drive\n",
        "\n",
        "Copy your pre-downloaded model from Google Drive to the model directory.\n",
        "\n",
        "**Configured Path:** `My Drive/Xiyan-SQL/Models/Qwen/`\n",
        "\n",
        "The script will automatically detect and copy the model folder(s) from this location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_model"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Path to your model in Google Drive\n",
        "MODEL_DRIVE_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Models/Qwen\"\n",
        "\n",
        "# Target directory in the repository\n",
        "MODEL_TARGET_DIR = \"train/model/Qwen\"\n",
        "\n",
        "# Create target directory if it doesn't exist\n",
        "os.makedirs(MODEL_TARGET_DIR, exist_ok=True)\n",
        "\n",
        "# Check if model directory exists in Drive\n",
        "if os.path.exists(MODEL_DRIVE_PATH):\n",
        "    print(f\"üì• Found model directory at {MODEL_DRIVE_PATH}\")\n",
        "    \n",
        "    # List contents to see what's inside\n",
        "    contents = os.listdir(MODEL_DRIVE_PATH)\n",
        "    print(f\"üìÅ Contents: {contents}\")\n",
        "    \n",
        "    # Check if it's a single model folder or contains multiple model folders\n",
        "    model_folders = [item for item in contents if os.path.isdir(os.path.join(MODEL_DRIVE_PATH, item))]\n",
        "    \n",
        "    if len(model_folders) == 1:\n",
        "        # Single model folder - copy it directly\n",
        "        model_name = model_folders[0]\n",
        "        source_path = os.path.join(MODEL_DRIVE_PATH, model_name)\n",
        "        target_path = os.path.join(MODEL_TARGET_DIR, model_name)\n",
        "        \n",
        "        if os.path.exists(target_path):\n",
        "            print(f\"‚ö†Ô∏è  Model already exists at {target_path}\")\n",
        "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
        "        else:\n",
        "            print(f\"üì• Copying model '{model_name}' from {source_path}...\")\n",
        "            shutil.copytree(source_path, target_path)\n",
        "            print(f\"‚úÖ Model copied to {target_path}\")\n",
        "        \n",
        "        MODEL_PATH = target_path\n",
        "    else:\n",
        "        # Multiple folders or files - copy the entire Qwen directory\n",
        "        target_path = MODEL_TARGET_DIR\n",
        "        if os.path.exists(target_path) and os.listdir(target_path):\n",
        "            print(f\"‚ö†Ô∏è  Model directory already exists at {target_path}\")\n",
        "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
        "        else:\n",
        "            print(f\"üì• Copying all models from {MODEL_DRIVE_PATH}...\")\n",
        "            for item in contents:\n",
        "                source_item = os.path.join(MODEL_DRIVE_PATH, item)\n",
        "                target_item = os.path.join(target_path, item)\n",
        "                if os.path.isdir(source_item):\n",
        "                    if not os.path.exists(target_item):\n",
        "                        shutil.copytree(source_item, target_item)\n",
        "                        print(f\"  ‚úÖ Copied {item}\")\n",
        "                else:\n",
        "                    if not os.path.exists(target_item):\n",
        "                        shutil.copy2(source_item, target_item)\n",
        "                        print(f\"  ‚úÖ Copied {item}\")\n",
        "            print(f\"‚úÖ All models copied to {target_path}\")\n",
        "        \n",
        "        # Set MODEL_PATH to the first model folder found, or let user specify\n",
        "        if model_folders:\n",
        "            MODEL_PATH = os.path.join(MODEL_TARGET_DIR, model_folders[0])\n",
        "            print(f\"\\nüìå Using model: {MODEL_PATH}\")\n",
        "            print(f\"üí° If you want to use a different model, update MODEL_PATH in Step 7\")\n",
        "        else:\n",
        "            MODEL_PATH = MODEL_TARGET_DIR\n",
        "            print(f\"\\nüìå Model directory: {MODEL_PATH}\")\n",
        "            print(f\"üí° Please specify the exact model folder name in Step 7\")\n",
        "    \n",
        "    print(f\"\\nüìå Model path for training: {MODEL_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ùå Model not found at {MODEL_DRIVE_PATH}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. Google Drive is mounted correctly\")\n",
        "    print(\"2. The path 'My Drive/Xiyan-SQL/Models/Qwen/' exists in your Drive\")\n",
        "    MODEL_PATH = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step5"
      },
      "source": [
        "## Step 5: Copy and Extract Dataset from Google Drive\n",
        "\n",
        "Copy and extract your dataset zip file from Google Drive.\n",
        "\n",
        "**Configured Path:** `My Drive/Xiyan-SQL/Dataset/train.zip`\n",
        "\n",
        "The script will automatically:\n",
        "1. Copy the zip file from Google Drive\n",
        "2. Extract it to the appropriate location\n",
        "3. Verify the extraction was successful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "copy_dataset"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "# Path to your dataset zip file in Google Drive\n",
        "DATASET_ZIP_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Dataset/train.zip\"\n",
        "\n",
        "# Target directory\n",
        "DATA_TARGET_DIR = \"data/data_warehouse\"\n",
        "os.makedirs(DATA_TARGET_DIR, exist_ok=True)\n",
        "\n",
        "# Check if zip file exists\n",
        "if os.path.exists(DATASET_ZIP_PATH):\n",
        "    print(f\"üì• Found dataset zip file at {DATASET_ZIP_PATH}\")\n",
        "    \n",
        "    # Check if already extracted\n",
        "    extracted_path = os.path.join(DATA_TARGET_DIR, \"train\")\n",
        "    \n",
        "    if os.path.exists(extracted_path) and os.listdir(extracted_path):\n",
        "        print(f\"‚úÖ Dataset already extracted at {extracted_path}\")\n",
        "        print(\"Skipping extraction (delete the folder manually if you want to re-extract)\")\n",
        "    else:\n",
        "        # Copy zip file to local directory first (faster extraction)\n",
        "        local_zip = \"/content/train.zip\"\n",
        "        print(f\"üì• Copying zip file to local storage...\")\n",
        "        shutil.copy2(DATASET_ZIP_PATH, local_zip)\n",
        "        print(f\"‚úÖ Zip file copied\")\n",
        "        \n",
        "        # Extract zip file\n",
        "        print(f\"üì¶ Extracting dataset from {local_zip}...\")\n",
        "        print(\"‚è≥ This may take a few minutes depending on file size...\")\n",
        "        \n",
        "        with zipfile.ZipFile(local_zip, 'r') as zip_ref:\n",
        "            # Get the root directory name from the zip\n",
        "            zip_contents = zip_ref.namelist()\n",
        "            if zip_contents:\n",
        "                # Determine extraction path\n",
        "                # If zip contains a 'train' folder, extract to data_warehouse\n",
        "                # If zip contains data_warehouse structure, extract accordingly\n",
        "                first_item = zip_contents[0]\n",
        "                if 'data_warehouse' in first_item:\n",
        "                    # Extract maintaining structure\n",
        "                    zip_ref.extractall(DATA_TARGET_DIR)\n",
        "                    print(f\"‚úÖ Dataset extracted to {DATA_TARGET_DIR}\")\n",
        "                elif 'train' in first_item:\n",
        "                    # Extract train folder to data_warehouse\n",
        "                    zip_ref.extractall(DATA_TARGET_DIR)\n",
        "                    print(f\"‚úÖ Dataset extracted to {DATA_TARGET_DIR}\")\n",
        "                else:\n",
        "                    # Extract and create train folder\n",
        "                    zip_ref.extractall(extracted_path)\n",
        "                    print(f\"‚úÖ Dataset extracted to {extracted_path}\")\n",
        "        \n",
        "        # Clean up local zip file\n",
        "        if os.path.exists(local_zip):\n",
        "            os.remove(local_zip)\n",
        "        \n",
        "        print(f\"\\n‚úÖ Dataset extraction completed!\")\n",
        "        print(f\"üìÅ Check contents at: {DATA_TARGET_DIR}\")\n",
        "    \n",
        "    # Verify extraction\n",
        "    if os.path.exists(extracted_path):\n",
        "        contents = os.listdir(extracted_path)\n",
        "        print(f\"\\nüìã Extracted contents: {contents[:10]}...\" if len(contents) > 10 else f\"\\nüìã Extracted contents: {contents}\")\n",
        "else:\n",
        "    print(f\"‚ùå Dataset zip file not found at {DATASET_ZIP_PATH}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. Google Drive is mounted correctly\")\n",
        "    print(\"2. The file 'My Drive/Xiyan-SQL/Dataset/train.zip' exists in your Drive\")\n",
        "    print(\"3. The file name matches exactly (case-sensitive)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step6"
      },
      "source": [
        "## Step 6: Prepare Training Data\n",
        "\n",
        "If you have raw data, process it first. If you have processed data, assemble it into training format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "process_data"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import glob\n",
        "\n",
        "# Set training directory\n",
        "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
        "os.chdir(TRAINING_DIR)\n",
        "\n",
        "# Find dataset paths (handle different extraction structures)\n",
        "DATA_WAREHOUSE_DIR = \"data/data_warehouse\"\n",
        "\n",
        "# Check for processed data in various possible locations\n",
        "PROCESSED_DATA_PATTERNS = [\n",
        "    \"data/data_warehouse/train/processed_data/train_nl2sqlite.json\",\n",
        "    \"data/data_warehouse/processed_data/train_nl2sqlite.json\",\n",
        "    \"data/data_warehouse/*/processed_data/*.json\",\n",
        "]\n",
        "\n",
        "# Check for raw data in various possible locations\n",
        "RAW_DATA_PATTERNS = [\n",
        "    \"data/data_warehouse/train/raw_data/train.json\",\n",
        "    \"data/data_warehouse/raw_data/train.json\",\n",
        "    \"data/data_warehouse/*/raw_data/*.json\",\n",
        "]\n",
        "\n",
        "PROCESSED_DATA_PATH = None\n",
        "RAW_DATA_PATH = None\n",
        "\n",
        "# Find processed data\n",
        "for pattern in PROCESSED_DATA_PATTERNS:\n",
        "    matches = glob.glob(pattern)\n",
        "    if matches:\n",
        "        PROCESSED_DATA_PATH = matches[0]\n",
        "        break\n",
        "\n",
        "# Find raw data\n",
        "for pattern in RAW_DATA_PATTERNS:\n",
        "    matches = glob.glob(pattern)\n",
        "    if matches:\n",
        "        RAW_DATA_PATH = matches[0]\n",
        "        break\n",
        "\n",
        "DB_CONN_CONFIG = \"data/data_warehouse/train/db_conn.json\"  # ‚ö†Ô∏è You may need to create this\n",
        "\n",
        "print(f\"üìä Dataset search results:\")\n",
        "print(f\"  Processed data: {PROCESSED_DATA_PATH if PROCESSED_DATA_PATH else 'Not found'}\")\n",
        "print(f\"  Raw data: {RAW_DATA_PATH if RAW_DATA_PATH else 'Not found'}\")\n",
        "\n",
        "# Step 6a: Process raw data (if raw data exists and processed data doesn't)\n",
        "if RAW_DATA_PATH and os.path.exists(RAW_DATA_PATH) and (not PROCESSED_DATA_PATH or not os.path.exists(PROCESSED_DATA_PATH)):\n",
        "    print(\"üìä Processing raw data...\")\n",
        "    print(\"‚ö†Ô∏è  Note: You need db_conn.json for database connections\")\n",
        "    \n",
        "    # Create directories\n",
        "    os.makedirs(\"data/data_warehouse/train/processed_data\", exist_ok=True)\n",
        "    os.makedirs(\"data/data_warehouse/train/mschema\", exist_ok=True)\n",
        "    \n",
        "    # Run data processing\n",
        "    # Note: You may need to create db_conn.json first\n",
        "    cmd = [\n",
        "        \"python\", \"data/data_processing.py\",\n",
        "        \"--raw_data_path\", RAW_DATA_PATH,\n",
        "        \"--db_conn_config\", DB_CONN_CONFIG,\n",
        "        \"--processed_data_dir\", \"data/data_warehouse/train/processed_data/\",\n",
        "        \"--save_mschema_dir\", \"data/data_warehouse/train/mschema/\",\n",
        "        \"--save_to_configs\", \"data/configs/datasets_all.json\"\n",
        "    ]\n",
        "    \n",
        "    try:\n",
        "        result = subprocess.run(cmd, cwd=TRAINING_DIR, check=True, capture_output=True, text=True)\n",
        "        print(result.stdout)\n",
        "        if result.stderr:\n",
        "            print(\"Warnings:\", result.stderr)\n",
        "        print(\"‚úÖ Raw data processed successfully\")\n",
        "    except subprocess.CalledProcessError as e:\n",
        "        print(f\"‚ùå Error processing data: {e}\")\n",
        "        print(f\"Error output: {e.stderr}\")\n",
        "        print(\"\\n‚ö†Ô∏è  If you get database connection errors, you can skip this step if you already have processed data\")\n",
        "elif PROCESSED_DATA_PATH and os.path.exists(PROCESSED_DATA_PATH):\n",
        "    print(f\"‚úÖ Processed data already exists at {PROCESSED_DATA_PATH}, skipping processing step\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No raw or processed data found.\")\n",
        "    print(\"Please check:\")\n",
        "    print(\"1. The dataset zip was extracted correctly in Step 5\")\n",
        "    print(\"2. The zip file contains the expected folder structure\")\n",
        "    print(\"3. If you have processed data, proceed to data assembly step\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "assemble_data"
      },
      "outputs": [],
      "source": [
        "# Step 6b: Assemble training dataset\n",
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "import glob\n",
        "\n",
        "# Set training directory\n",
        "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
        "os.chdir(TRAINING_DIR)\n",
        "\n",
        "# Find processed data (handle different extraction structures)\n",
        "PROCESSED_DATA_PATTERNS = [\n",
        "    \"data/data_warehouse/train/processed_data/train_nl2sqlite.json\",\n",
        "    \"data/data_warehouse/processed_data/train_nl2sqlite.json\",\n",
        "    \"data/data_warehouse/*/processed_data/*.json\",\n",
        "]\n",
        "\n",
        "PROCESSED_DATA_PATH = None\n",
        "for pattern in PROCESSED_DATA_PATTERNS:\n",
        "    matches = glob.glob(pattern)\n",
        "    if matches:\n",
        "        PROCESSED_DATA_PATH = matches[0]\n",
        "        break\n",
        "\n",
        "TRAIN_DATASET_PATH = \"train/datasets/nl2sql_standard_train.json\"\n",
        "\n",
        "if PROCESSED_DATA_PATH and os.path.exists(PROCESSED_DATA_PATH):\n",
        "    # Create dataset config if it doesn't exist\n",
        "    dataset_config_path = \"data/configs/datasets_nl2sql_standard.json\"\n",
        "    \n",
        "    if not os.path.exists(dataset_config_path):\n",
        "        # Create a simple dataset config\n",
        "        config = {\n",
        "            \"train_data\": {\n",
        "                \"data_path\": PROCESSED_DATA_PATH,\n",
        "                \"sample_num\": -1,  # Use all samples\n",
        "                \"task_name\": \"nl2sqlite\",\n",
        "                \"data_aug\": False\n",
        "            }\n",
        "        }\n",
        "        os.makedirs(\"data/configs\", exist_ok=True)\n",
        "        with open(dataset_config_path, 'w') as f:\n",
        "            json.dump(config, f, indent=2)\n",
        "        print(f\"‚úÖ Created dataset config at {dataset_config_path}\")\n",
        "    \n",
        "    # Assemble training dataset\n",
        "    if not os.path.exists(TRAIN_DATASET_PATH):\n",
        "        print(\"üì¶ Assembling training dataset...\")\n",
        "        os.makedirs(\"train/datasets\", exist_ok=True)\n",
        "        \n",
        "        cmd = [\n",
        "            \"python\", \"data/data_assembler.py\",\n",
        "            \"--dataset_config_path\", dataset_config_path,\n",
        "            \"--save_path\", TRAIN_DATASET_PATH\n",
        "        ]\n",
        "        \n",
        "        try:\n",
        "            result = subprocess.run(cmd, cwd=TRAINING_DIR, check=True, capture_output=True, text=True)\n",
        "            print(result.stdout)\n",
        "            if result.stderr:\n",
        "                print(\"Warnings:\", result.stderr)\n",
        "            print(f\"‚úÖ Training dataset assembled at {TRAIN_DATASET_PATH}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            print(f\"‚ùå Error assembling data: {e}\")\n",
        "            print(f\"Error output: {e.stderr}\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Training dataset already exists at {TRAIN_DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Processed data not found.\")\n",
        "    print(\"Please check:\")\n",
        "    print(\"1. The dataset zip was extracted correctly in Step 5\")\n",
        "    print(\"2. The zip file contains processed data files\")\n",
        "    print(\"3. If you only have raw data, you may need to process it first (Step 6a)\")\n",
        "    print(\"4. Or manually specify the processed data path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step7"
      },
      "source": [
        "## Step 7: Configure Training Parameters\n",
        "\n",
        "Set up your training configuration. Adjust these parameters based on your GPU memory and requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_training"
      },
      "outputs": [],
      "source": [
        "# Training Configuration\n",
        "# ‚ö†Ô∏è Adjust these parameters based on your GPU memory\n",
        "\n",
        "TRAINING_CONFIG = {\n",
        "    # Experiment ID\n",
        "    \"expr_id\": \"nl2sql_3b_colab\",\n",
        "    \n",
        "    # Model path (set in Step 4)\n",
        "    \"model_path\": MODEL_PATH if 'MODEL_PATH' in globals() else \"train/model/Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
        "    \n",
        "    # Dataset path\n",
        "    \"data_path\": \"train/datasets/nl2sql_standard_train.json\",\n",
        "    \n",
        "    # Output directory\n",
        "    \"output_dir\": \"train/output/dense/nl2sql_3b_colab/\",\n",
        "    \n",
        "    # Training hyperparameters\n",
        "    \"epochs\": 5,\n",
        "    \"learning_rate\": 2e-6,\n",
        "    \"weight_decay\": 0.1,\n",
        "    \"max_length\": 10240,  # Reduce if OOM: 8192 or 4096\n",
        "    \n",
        "    # LoRA configuration (recommended for Colab)\n",
        "    \"use_lora\": True,\n",
        "    \"lora_r\": 512,\n",
        "    \"lora_alpha\": 512,  # Usually same as lora_r\n",
        "    \n",
        "    # Batch configuration (adjust for your GPU)\n",
        "    \"batch_size\": 1,  # Start with 1, increase if memory allows\n",
        "    \"gradient_accumulation_steps\": 4,  # Effective batch = batch_size * grad_accum * num_gpus\n",
        "    \n",
        "    # Other settings\n",
        "    \"save_steps\": 500,\n",
        "    \"group_by_length\": True,\n",
        "    \"shuffle\": True,\n",
        "    \"use_flash_attention\": True,\n",
        "    \"bf16\": True,\n",
        "}\n",
        "\n",
        "print(\"üìã Training Configuration:\")\n",
        "for key, value in TRAINING_CONFIG.items():\n",
        "    print(f\"  {key}: {value}\")\n",
        "\n",
        "print(\"\\nüí° Tips:\")\n",
        "print(\"  - If you get OOM (Out of Memory) errors:\")\n",
        "print(\"    * Reduce batch_size to 1\")\n",
        "print(\"    * Reduce max_length to 8192 or 4096\")\n",
        "print(\"    * Increase gradient_accumulation_steps\")\n",
        "print(\"  - For faster training:\")\n",
        "print(\"    * Increase batch_size if memory allows\")\n",
        "print(\"    * Reduce max_length if not needed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step8"
      },
      "source": [
        "## Step 8: Start Training\n",
        "\n",
        "Run the training script with your configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "train_model"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import json\n",
        "\n",
        "# Set training directory\n",
        "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
        "os.chdir(TRAINING_DIR)\n",
        "\n",
        "# Create DeepSpeed config for single GPU (Colab typically has 1 GPU)\n",
        "ds_config = {\n",
        "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
        "    \"distributed_type\": \"DEEPSPEED\",\n",
        "    \"deepspeed_config\": {\n",
        "        \"gradient_accumulation_steps\": TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
        "        \"gradient_clipping\": 1.0,\n",
        "        \"offload_optimizer_device\": \"cpu\",  # Offload to CPU to save GPU memory\n",
        "        \"offload_param_device\": \"cpu\",\n",
        "        \"zero3_init_flag\": False,\n",
        "        \"zero3_save_16bit_model\": False,\n",
        "        \"zero_stage\": 2,  # Use Zero2 for efficiency\n",
        "        \"bf16\": {\n",
        "            \"enabled\": True\n",
        "        }\n",
        "    },\n",
        "    \"machine_rank\": 0,\n",
        "    \"main_process_ip\": None,\n",
        "    \"main_process_port\": None,\n",
        "    \"num_machines\": 1,\n",
        "    \"num_processes\": 1,  # Single GPU in Colab\n",
        "    \"rdzv_backend\": \"static\",\n",
        "    \"same_network\": True,\n",
        "    \"tpu_env\": [],\n",
        "    \"tpu_use_cluster\": False,\n",
        "    \"tpu_use_sudo\": False,\n",
        "    \"use_cpu\": False\n",
        "}\n",
        "\n",
        "# Save DeepSpeed config\n",
        "os.makedirs(\"train/config\", exist_ok=True)\n",
        "ds_config_path = \"train/config/colab_zero2.json\"\n",
        "with open(ds_config_path, 'w') as f:\n",
        "    json.dump(ds_config, f, indent=2)\n",
        "\n",
        "print(\"üöÄ Starting training...\")\n",
        "print(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\n",
        "print(f\"üìä Dataset: {TRAINING_CONFIG['data_path']}\")\n",
        "print(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\n",
        "print(\"\\n‚è≥ This may take several hours depending on dataset size...\")\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "\n",
        "# Build training command\n",
        "cmd = [\n",
        "    \"accelerate\", \"launch\",\n",
        "    \"--config_file\", ds_config_path,\n",
        "    \"--num_processes\", \"1\",\n",
        "    \"train/sft4xiyan.py\",\n",
        "    \"--save_only_model\", \"True\",\n",
        "    \"--resume\", \"False\",\n",
        "    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n",
        "    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n",
        "    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n",
        "    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n",
        "    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
        "    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n",
        "    \"--save_strategy\", \"steps\",\n",
        "    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n",
        "    \"--save_total_limit\", \"3\",  # Keep only last 3 checkpoints\n",
        "    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
        "    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
        "    \"--adam_beta2\", \"0.95\",\n",
        "    \"--warmup_ratio\", \"0.1\",\n",
        "    \"--lr_scheduler_type\", \"cosine\",\n",
        "    \"--logging_steps\", \"10\",\n",
        "    \"--report_to\", \"none\",\n",
        "    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n",
        "    \"--lazy_preprocess\", \"False\",\n",
        "    \"--gradient_checkpointing\", \"True\",\n",
        "    \"--predict_with_generate\", \"True\",\n",
        "    \"--include_inputs_for_metrics\", \"True\",\n",
        "    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n",
        "    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n",
        "    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n",
        "    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n",
        "    \"--torch_compile\", \"False\",\n",
        "    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n",
        "    \"--model_type\", \"auto\",\n",
        "    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n",
        "    \"--bf16\",\n",
        "    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n",
        "]\n",
        "\n",
        "# Run training\n",
        "try:\n",
        "    result = subprocess.run(\n",
        "        cmd,\n",
        "        cwd=TRAINING_DIR,\n",
        "        check=False  # Don't raise on error, we'll check return code\n",
        "    )\n",
        "    \n",
        "    if result.returncode == 0:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"‚úÖ Training completed successfully!\")\n",
        "        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(f\"‚ùå Training failed with return code {result.returncode}\")\n",
        "        print(\"\\nCommon issues:\")\n",
        "        print(\"  - Out of Memory (OOM): Reduce batch_size or max_length\")\n",
        "        print(\"  - Model not found: Check MODEL_PATH in Step 4\")\n",
        "        print(\"  - Dataset not found: Check data_path in Step 6\")\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error during training: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "step9"
      },
      "source": [
        "## Step 9: Save Trained Model to Google Drive (Optional)\n",
        "\n",
        "After training completes, save your model to Google Drive for future use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "save_to_drive"
      },
      "outputs": [],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Path to trained model\n",
        "TRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
        "\n",
        "# Destination in Google Drive\n",
        "# ‚ö†Ô∏è UPDATE THIS: Where you want to save the trained model\n",
        "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/trained_models/nl2sql_3b_colab\"  # ‚ö†Ô∏è UPDATE THIS\n",
        "\n",
        "if os.path.exists(TRAINED_MODEL_PATH):\n",
        "    print(f\"üì• Copying trained model to Google Drive...\")\n",
        "    print(f\"   From: {TRAINED_MODEL_PATH}\")\n",
        "    print(f\"   To: {DRIVE_SAVE_PATH}\")\n",
        "    \n",
        "    # Create parent directory\n",
        "    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n",
        "    \n",
        "    # Copy model\n",
        "    if os.path.exists(DRIVE_SAVE_PATH):\n",
        "        shutil.rmtree(DRIVE_SAVE_PATH)\n",
        "    \n",
        "    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n",
        "    print(f\"\\n‚úÖ Model saved to Google Drive: {DRIVE_SAVE_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n",
        "    print(\"Make sure training completed successfully in Step 8.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "troubleshooting"
      },
      "source": [
        "## Troubleshooting\n",
        "\n",
        "### Out of Memory (OOM) Errors\n",
        "- Reduce `batch_size` to 1\n",
        "- Reduce `max_length` to 8192 or 4096\n",
        "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
        "- The DeepSpeed config already uses CPU offloading, which helps\n",
        "\n",
        "### Model Not Found\n",
        "- Check that `MODEL_DRIVE_PATH` in Step 4 is correct\n",
        "- Verify the model folder exists in Google Drive\n",
        "- Ensure the model folder contains all required files (config.json, tokenizer files, etc.)\n",
        "\n",
        "### Dataset Not Found\n",
        "- Check that dataset paths in Step 5 are correct\n",
        "- Verify files exist in Google Drive\n",
        "- If processing raw data, ensure `db_conn.json` exists\n",
        "\n",
        "### Training Too Slow\n",
        "- Colab free tier has limited GPU time\n",
        "- Consider using Colab Pro for longer training sessions\n",
        "- Reduce dataset size for testing (set `sample_num` in dataset config)\n",
        "\n",
        "### Connection Issues\n",
        "- Colab sessions may disconnect after inactivity\n",
        "- Use `nohup` or save checkpoints frequently\n",
        "- Consider running training in multiple sessions if needed"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
