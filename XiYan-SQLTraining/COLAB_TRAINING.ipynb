{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# XiYan-SQL Training on Google Colab\n",
    "\n",
    "This notebook provides a complete step-by-step guide to train the XiYan-SQL model on Google Colab.\n",
    "\n",
    "## Prerequisites\n",
    "- Upload your model files to Google Drive (e.g., `Qwen2.5-Coder-3B-Instruct` folder)\n",
    "- Upload your dataset files to Google Drive (raw data, processed data, or both)\n",
    "- Enable GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required packages for XiYan-SQL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libaio-dev  # Required for DeepSpeed\n",
    "\n",
    "print(\"üì¶ Installing Python packages...\")\n",
    "print(\"‚ö†Ô∏è  Note: Installing in specific order to avoid numpy/DeepSpeed conflicts.\\n\")\n",
    "\n",
    "# Install DeepSpeed AFTER numpy and torch are set\n",
    "print(\"\\nüîß Installing DeepSpeed (may show some warnings)...\")\n",
    "!pip install -q --disable-pip-version-check --no-cache-dir deepspeed\n",
    "\n",
    "# Install remaining packages\n",
    "!pip install -q --disable-pip-version-check llama-index>=0.9.6.post2\n",
    "!pip install -q --disable-pip-version-check modelscope>=1.33.0\n",
    "!pip install -q --disable-pip-version-check mysql-connector-python>=9.5.0\n",
    "!pip install -q --disable-pip-version-check \"protobuf>=6.33.3\"\n",
    "!pip install -q --disable-pip-version-check psycopg2-binary>=2.9.11\n",
    "!pip install -q --disable-pip-version-check swanlab>=0.7.6\n",
    "!pip install -q --disable-pip-version-check textdistance>=4.6.3\n",
    "!pip install -q --disable-pip-version-check jedi>=0.16\n",
    "\n",
    "# Install flash-attn (optional, for faster attention)\n",
    "print(\"\\nüî® Attempting to install flash-attn (this may take a few minutes)...\")\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"pip\", \"install\", \"-q\", \"--no-build-isolation\", \"flash-attn\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ flash-attn installed successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  flash-attn installation failed (this is optional, continuing without it)\")\n",
    "\n",
    "print(\"\\n‚úÖ Core dependencies installed!\")\n",
    "print(\"\\nüí° If you see numpy warnings, they are expected and won't affect training.\")\n",
    "\n",
    "# Verify installation\n",
    "print(\"\\nüîç Verifying installation...\")\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import accelerate\n",
    "    import deepspeed\n",
    "    import peft\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "    print(f\"‚úÖ Accelerate: {accelerate.__version__}\")\n",
    "    print(f\"‚úÖ DeepSpeed: {deepspeed.__version__}\")\n",
    "    print(f\"‚úÖ PEFT: {peft.__version__}\")\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"‚úÖ GPU Memory: {gpu_mem:.1f} GB\")\n",
    "\n",
    "        if gpu_mem >= 14:\n",
    "            print(\"\\nüéØ Your GPU has 15GB+ memory - perfect for optimized training!\")\n",
    "        elif gpu_mem >= 10:\n",
    "            print(\"\\nüìä Your GPU has 12GB memory - good for moderate training.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Your GPU has limited memory - training will use conservative settings.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No GPU detected! Make sure to enable GPU in Runtime ‚Üí Change runtime type\")\n",
    "\n",
    "    print(\"\\nüöÄ Ready to proceed!\")\n",
    "\n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîÑ If you see numpy errors, restart runtime and run this cell again.\")\n",
    "    print(\"   Go to: Runtime ‚Üí Restart runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Login to SwanLab (Optional but Recommended)\n",
    "\n",
    "SwanLab will automatically track your training metrics, including:\n",
    "- Training loss\n",
    "- Learning rate\n",
    "- GPU/CPU/RAM usage\n",
    "- All hyperparameters\n",
    "- Model checkpoints\n",
    "\n",
    "**Get your API key:** https://swanlab.cn (sign up/login, then go to Settings ‚Üí API Key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!swanlab login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Clone Repository\n",
    "\n",
    "Clone the XiYan-SQL repository to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Change to content directory\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone the repository\n",
    "# Replace with your repository URL\n",
    "REPO_URL = \"https://github.com/rezaarrazi/XiYan-SQL.git\"  # ‚ö†Ô∏è UPDATE THIS\n",
    "\n",
    "if not os.path.exists('XiYan-SQL'):\n",
    "    os.system(f'git clone {REPO_URL}')\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Navigate to training directory\n",
    "os.chdir('XiYan-SQL/XiYan-SQLTraining')\n",
    "\n",
    "# Add to Python path so imports work correctly\n",
    "TRAINING_DIR = os.getcwd()\n",
    "if TRAINING_DIR not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_DIR)\n",
    "if os.path.dirname(TRAINING_DIR) not in sys.path:\n",
    "    sys.path.insert(0, os.path.dirname(TRAINING_DIR))\n",
    "\n",
    "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access model and dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully\")\n",
    "print(\"\\nüìÇ Drive path: /content/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Copy Model from Google Drive\n",
    "\n",
    "Copy your pre-downloaded model from Google Drive to the model directory.\n",
    "\n",
    "**Configured Path:** `My Drive/Xiyan-SQL/Models/Qwen/`\n",
    "\n",
    "The script will automatically detect and copy the model folder(s) from this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_model"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to your model in Google Drive\n",
    "MODEL_DRIVE_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Models/Qwen\"\n",
    "\n",
    "# Target directory in the repository\n",
    "MODEL_TARGET_DIR = \"train/model/Qwen\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(MODEL_TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model directory exists in Drive\n",
    "if os.path.exists(MODEL_DRIVE_PATH):\n",
    "    print(f\"üì• Found model directory at {MODEL_DRIVE_PATH}\")\n",
    "    \n",
    "    # List contents to see what's inside\n",
    "    contents = os.listdir(MODEL_DRIVE_PATH)\n",
    "    print(f\"üìÅ Contents: {contents}\")\n",
    "    \n",
    "    # Check if it's a single model folder or contains multiple model folders\n",
    "    model_folders = [item for item in contents if os.path.isdir(os.path.join(MODEL_DRIVE_PATH, item))]\n",
    "    \n",
    "    if len(model_folders) == 1:\n",
    "        # Single model folder - copy it directly\n",
    "        model_name = model_folders[0]\n",
    "        source_path = os.path.join(MODEL_DRIVE_PATH, model_name)\n",
    "        target_path = os.path.join(MODEL_TARGET_DIR, model_name)\n",
    "        \n",
    "        if os.path.exists(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying model '{model_name}' from {source_path}...\")\n",
    "            shutil.copytree(source_path, target_path)\n",
    "            print(f\"‚úÖ Model copied to {target_path}\")\n",
    "        \n",
    "        MODEL_PATH = target_path\n",
    "    else:\n",
    "        # Multiple folders or files - copy the entire Qwen directory\n",
    "        target_path = MODEL_TARGET_DIR\n",
    "        if os.path.exists(target_path) and os.listdir(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model directory already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying all models from {MODEL_DRIVE_PATH}...\")\n",
    "            for item in contents:\n",
    "                source_item = os.path.join(MODEL_DRIVE_PATH, item)\n",
    "                target_item = os.path.join(target_path, item)\n",
    "                if os.path.isdir(source_item):\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copytree(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "                else:\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copy2(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "            print(f\"‚úÖ All models copied to {target_path}\")\n",
    "        \n",
    "        # Set MODEL_PATH to the first model folder found, or let user specify\n",
    "        if model_folders:\n",
    "            MODEL_PATH = os.path.join(MODEL_TARGET_DIR, model_folders[0])\n",
    "            print(f\"\\nüìå Using model: {MODEL_PATH}\")\n",
    "            print(f\"üí° If you want to use a different model, update MODEL_PATH in Step 7\")\n",
    "        else:\n",
    "            MODEL_PATH = MODEL_TARGET_DIR\n",
    "            print(f\"\\nüìå Model directory: {MODEL_PATH}\")\n",
    "            print(f\"üí° Please specify the exact model folder name in Step 7\")\n",
    "    \n",
    "    print(f\"\\nüìå Model path for training: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {MODEL_DRIVE_PATH}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. Google Drive is mounted correctly\")\n",
    "    print(\"2. The path 'My Drive/Xiyan-SQL/Models/Qwen/' exists in your Drive\")\n",
    "    MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "## Step 5: Verify Training Dataset\n",
    "\n",
    "The English training dataset should already be in the repository (via Git LFS).\n",
    "\n",
    "**Expected file:** `train/datasets/nl2sql_standard_train_en.json` (55MB)\n",
    "\n",
    "If the file is not present, you can download it from Google Drive as a backup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Check if training dataset exists in repository\n",
    "TRAIN_DATASET_PATH = \"train/datasets/nl2sql_standard_train_en.json\"\n",
    "\n",
    "if os.path.exists(TRAIN_DATASET_PATH):\n",
    "    print(f\"‚úÖ Training dataset found in repository!\")\n",
    "    print(f\"   Path: {TRAIN_DATASET_PATH}\")\n",
    "    \n",
    "    size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n",
    "    print(f\"   Size: {size_mb:.1f} MB\")\n",
    "    \n",
    "    # Quick verification\n",
    "    with open(TRAIN_DATASET_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "        print(f\"   Samples: {len(data)}\")\n",
    "        \n",
    "        # Check if English\n",
    "        if data and data[0].get('conversations'):\n",
    "            prompt = data[0]['conversations'][0]['content']\n",
    "            if prompt.startswith(\"You are a SQLite expert\"):\n",
    "                print(f\"   Language: ‚úÖ English\")\n",
    "            else:\n",
    "                print(f\"   Language: ‚ö†Ô∏è Not English\")\n",
    "    \n",
    "    print(\"\\nüéâ Ready to start training! Skip to Step 6.\")\n",
    "    \n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Training dataset not found in repository\")\n",
    "    print(f\"   Expected: {TRAIN_DATASET_PATH}\")\n",
    "    print(\"\\nüì• Attempting to download from Google Drive as backup...\")\n",
    "    \n",
    "    # Backup: Download from Google Drive\n",
    "    DRIVE_DATASET_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Dataset/nl2sql_standard_train_en.json\"\n",
    "    \n",
    "    if os.path.exists(DRIVE_DATASET_PATH):\n",
    "        import shutil\n",
    "        os.makedirs(\"train/datasets\", exist_ok=True)\n",
    "        shutil.copy2(DRIVE_DATASET_PATH, TRAIN_DATASET_PATH)\n",
    "        print(f\"‚úÖ Dataset copied from Google Drive\")\n",
    "        \n",
    "        size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n",
    "        print(f\"   Size: {size_mb:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"‚ùå Dataset not found in Google Drive either\")\n",
    "        print(f\"   Expected: {DRIVE_DATASET_PATH}\")\n",
    "        print(\"\\nüí° Options:\")\n",
    "        print(\"1. Make sure Git LFS pulled the dataset when cloning\")\n",
    "        print(\"2. Upload nl2sql_standard_train_en.json to Google Drive\")\n",
    "        print(\"3. Or run data processing from BIRD raw data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": [
    "## Step 6: Configure Training Parameters\n",
    "\n",
    "**Auto-detects your GPU and optimizes settings:**\n",
    "- A100/H100 40GB+: High-end config (16K context, batch 6-8, LoRA rank 256)\n",
    "- L4 24GB: High performance (12K context, batch 3, LoRA rank 128)  \n",
    "- T4 15GB: Balanced config (4K context, batch 1, LoRA rank 64)\n",
    "\n",
    "Flash Attention is automatically enabled for Ampere+ GPUs (A100, H100, L4) and disabled for T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available GPU memory\n",
    "import subprocess\n",
    "import re\n",
    "import os\n",
    "\n",
    "# Set CUDA memory configuration to reduce fragmentation\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "try:\n",
    "    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], \n",
    "                          capture_output=True, text=True)\n",
    "    gpu_memory_mb = int(result.stdout.strip())\n",
    "    gpu_memory_gb = gpu_memory_mb / 1024\n",
    "    print(f\"üéÆ Detected GPU Memory: {gpu_memory_gb:.1f} GB\")\n",
    "    \n",
    "    # Detect GPU architecture for flash attention compatibility\n",
    "    gpu_name_result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], \n",
    "                                     capture_output=True, text=True)\n",
    "    gpu_name = gpu_name_result.stdout.strip()\n",
    "    print(f\"üéÆ GPU: {gpu_name}\")\n",
    "    \n",
    "    # Flash attention only works on Ampere or newer (A100, A10, RTX 3090, RTX 4090, etc.)\n",
    "    # T4 is Turing architecture - doesn't support flash attention\n",
    "    supports_flash = any(x in gpu_name.upper() for x in ['A100', 'A10', 'RTX 30', 'RTX 40', 'H100', 'L4'])\n",
    "    if not supports_flash and 'T4' in gpu_name:\n",
    "        print(f\"‚ö†Ô∏è  T4 GPU detected - Flash Attention will be disabled (T4 is Turing, needs Ampere+)\")\n",
    "    elif supports_flash:\n",
    "        print(f\"‚úÖ Flash Attention supported on this GPU!\")\n",
    "except:\n",
    "    gpu_memory_gb = 15.0  # Default assumption\n",
    "    supports_flash = False\n",
    "    print(f\"‚ö†Ô∏è  Could not detect GPU, assuming {gpu_memory_gb} GB\")\n",
    "    print(f\"‚ö†Ô∏è  Flash Attention will be disabled for compatibility\")\n",
    "\n",
    "# Auto-configure based on GPU memory - VERY CONSERVATIVE to avoid OOM\n",
    "if gpu_memory_gb >= 70:\n",
    "    # 80GB GPU (H100 80GB) - Ultra conservative (model alone uses 55GB!)\n",
    "    MAX_LENGTH = 8192      # Further reduced from 12288\n",
    "    LORA_R = 64            # Further reduced from 128\n",
    "    BATCH_SIZE = 2         # Further reduced from 4\n",
    "    GRAD_ACC = 16          # Effective batch = 32\n",
    "    print(f\"üî• Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU (H100 80GB)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Model uses ~55GB, being very conservative with remaining memory\")\n",
    "elif gpu_memory_gb >= 35:\n",
    "    # 40GB GPU (A100 40GB, H100 40GB) - Very conservative\n",
    "    MAX_LENGTH = 6144      # Further reduced from 8192\n",
    "    LORA_R = 64            # Further reduced from 128\n",
    "    BATCH_SIZE = 1         # Reduced from 2\n",
    "    GRAD_ACC = 32          # Effective batch = 32\n",
    "    print(f\"üöÄ Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU (A100/H100)\")\n",
    "    print(f\"   ‚ö†Ô∏è  Very conservative settings to avoid OOM\")\n",
    "elif gpu_memory_gb >= 22:\n",
    "    # 24GB GPU (L4, RTX 4090, A10) - Very conservative\n",
    "    MAX_LENGTH = 6144      # Reduced from 8192\n",
    "    LORA_R = 32            # Reduced from 64\n",
    "    BATCH_SIZE = 1         # Reduced from 2\n",
    "    GRAD_ACC = 32          # Effective batch = 32\n",
    "    print(f\"üéØ Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU\")\n",
    "    print(f\"   ‚ö†Ô∏è  Very conservative for stability\")\n",
    "elif gpu_memory_gb >= 14:\n",
    "    # 15GB GPU (T4, P100) - BALANCED\n",
    "    MAX_LENGTH = 4096      # Standard context\n",
    "    LORA_R = 64            # Balanced LoRA\n",
    "    BATCH_SIZE = 1         # Small batch\n",
    "    GRAD_ACC = 32          # Effective batch = 32\n",
    "    print(f\"üìä Using BALANCED config for {gpu_memory_gb:.1f}GB GPU\")\n",
    "    print(f\"   (Conservative settings to avoid OOM)\")\n",
    "elif gpu_memory_gb >= 10:\n",
    "    # 12GB GPU - LOW MEMORY\n",
    "    MAX_LENGTH = 2048\n",
    "    LORA_R = 32\n",
    "    BATCH_SIZE = 1\n",
    "    GRAD_ACC = 64\n",
    "    print(f\"üìä Using LOW MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\n",
    "else:\n",
    "    # 8GB GPU - ULTRA-LOW\n",
    "    MAX_LENGTH = 1024\n",
    "    LORA_R = 16\n",
    "    BATCH_SIZE = 1\n",
    "    GRAD_ACC = 128\n",
    "    print(f\"üìä Using ULTRA-LOW MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\n",
    "\n",
    "TRAINING_CONFIG = {\n",
    "    # Experiment ID\n",
    "    \"expr_id\": \"nl2sql_3b_colab_en\",\n",
    "    \n",
    "    # Model path (set in Step 4)\n",
    "    \"model_path\": MODEL_PATH if 'MODEL_PATH' in globals() else \"train/model/Qwen/Qwen2.5-Coder-3B-Instruct\",\n",
    "    \n",
    "    # Dataset path - Using English version\n",
    "    \"data_path\": \"train/datasets/nl2sql_standard_train_en.json\",\n",
    "    \n",
    "    # Output directory\n",
    "    \"output_dir\": \"train/output/dense/nl2sql_3b_colab_en/\",\n",
    "    \n",
    "    # Training hyperparameters\n",
    "    \"epochs\": 5 if gpu_memory_gb >= 35 else 3,  # More epochs for A100/H100\n",
    "    \"learning_rate\": 2e-5,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"max_length\": MAX_LENGTH,\n",
    "    \n",
    "    # LoRA configuration\n",
    "    \"use_lora\": True,\n",
    "    \"lora_r\": LORA_R,\n",
    "    \"lora_alpha\": LORA_R * 2,\n",
    "    \n",
    "    # Batch configuration\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"gradient_accumulation_steps\": GRAD_ACC,\n",
    "    \n",
    "    # Other settings\n",
    "    \"save_steps\": 200,\n",
    "    \"group_by_length\": True,\n",
    "    \"shuffle\": True,\n",
    "    \"use_flash_attention\": supports_flash,  # Auto-detect based on GPU\n",
    "    \"bf16\": True,\n",
    "}\n",
    "\n",
    "print(\"\\nüìã Training Configuration:\")\n",
    "print(f\"  Experiment ID: {TRAINING_CONFIG['expr_id']}\")\n",
    "print(f\"  Dataset: {TRAINING_CONFIG['data_path']}\")\n",
    "print(f\"  Max Length: {TRAINING_CONFIG['max_length']} tokens\")\n",
    "print(f\"  LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\n",
    "print(f\"  Batch Size: {TRAINING_CONFIG['batch_size']}\")\n",
    "print(f\"  Gradient Accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Effective Batch Size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\n",
    "print(f\"  Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n",
    "print(f\"  Flash Attention: {'‚úÖ Enabled' if TRAINING_CONFIG['use_flash_attention'] else '‚ùå Disabled (GPU not compatible)'}\")\n",
    "\n",
    "print(\"\\nüí° Estimated Training Time:\")\n",
    "samples = 9431\n",
    "steps_per_epoch = samples // (TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps'])\n",
    "total_steps = steps_per_epoch * TRAINING_CONFIG['epochs']\n",
    "\n",
    "# Estimate time based on GPU and configuration\n",
    "if gpu_memory_gb >= 70:\n",
    "    time_per_step_sec = 2.5  # H100 80GB - slower due to conservative settings\n",
    "elif gpu_memory_gb >= 35:\n",
    "    time_per_step_sec = 3.0  # A100/H100 40GB\n",
    "elif gpu_memory_gb >= 22:\n",
    "    time_per_step_sec = 3.5  # L4/A10\n",
    "elif supports_flash:\n",
    "    time_per_step_sec = 3.5  # Other GPUs with flash attention\n",
    "else:\n",
    "    time_per_step_sec = 4    # Without flash attention\n",
    "\n",
    "total_hours = (total_steps * time_per_step_sec) / 3600\n",
    "print(f\"  Steps per epoch: ~{steps_per_epoch}\")\n",
    "print(f\"  Total steps: ~{total_steps}\")\n",
    "print(f\"  Estimated time: ~{total_hours:.1f} hours\")\n",
    "\n",
    "if gpu_memory_gb >= 70:\n",
    "    print(f\"\\n‚ö†Ô∏è  H100 80GB Note:\")\n",
    "    print(f\"   Base model uses ~55GB alone!\")\n",
    "    print(f\"   Using very conservative settings: 8K context, batch 2, LoRA 64\")\n",
    "    print(f\"   Training will work but slower than expected\")\n",
    "elif gpu_memory_gb >= 35:\n",
    "    print(f\"\\n‚ö†Ô∏è  A100/H100 40GB:\")\n",
    "    print(f\"   Using very conservative settings to ensure stability\")\n",
    "\n",
    "print(\"\\n‚ö†Ô∏è  Colab Tips:\")\n",
    "if gpu_memory_gb >= 35:\n",
    "    print(\"  - Colab Pro+: 24 hour runtime limit with A100/H100\")\n",
    "    print(\"  - Training should complete well within time limit\")\n",
    "else:\n",
    "    print(\"  - Colab Pro: Longer runtime than free tier\")\n",
    "print(\"  - Keep browser tab active to prevent disconnection\")\n",
    "print(\"  - Enable background execution in Colab settings\")\n",
    "\n",
    "print(\"\\nüíæ Expected Memory Usage:\")\n",
    "if gpu_memory_gb >= 70:\n",
    "    print(f\"  ‚ö†Ô∏è  WARNING: Base model alone uses ~55GB!\")\n",
    "    print(f\"  - Ultra conservative: 8K context, batch 2, LoRA 64\")\n",
    "    print(f\"  - Expected total usage: ~65-70GB\")\n",
    "    print(f\"  - This leaves minimal headroom - training will be slow\")\n",
    "elif gpu_memory_gb >= 35:\n",
    "    print(f\"  - Very conservative: 6K context, batch 1, LoRA 64\")\n",
    "    print(f\"  - Expected usage: ~28-33GB\")\n",
    "elif gpu_memory_gb >= 22:\n",
    "    print(f\"  - Conservative: 6K context, batch 1, LoRA 32\")\n",
    "    print(f\"  - Expected usage: ~15-18GB\")\n",
    "else:\n",
    "    print(f\"  - Conservative settings to prevent OOM\")\n",
    "    print(f\"  - Expected usage: ~6-12GB\")\n",
    "\n",
    "print(\"\\nüîß Memory optimization: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n",
    "\n",
    "if gpu_memory_gb >= 70:\n",
    "    print(\"\\nüí° Tip: The 3B model is quite large in memory.\")\n",
    "    print(\"   Consider using smaller batch sizes and shorter context if OOM persists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assemble_data"
   },
   "source": [
    "## Step 7: Start Training\n",
    "\n",
    "Run the training with your optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Set training directory\n",
    "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
    "os.chdir(TRAINING_DIR)\n",
    "\n",
    "# Create DeepSpeed config YAML for single GPU (compatible with accelerate)\n",
    "# IMPORTANT: For 15GB GPU, we DON'T offload parameters to CPU - keep model on GPU!\n",
    "# Only offload optimizer states to save GPU memory\n",
    "ds_config_yaml = \"\"\"compute_environment: LOCAL_MACHINE\n",
    "distributed_type: DEEPSPEED\n",
    "deepspeed_config:\n",
    "  gradient_accumulation_steps: {grad_acc}\n",
    "  gradient_clipping: 1.0\n",
    "  offload_optimizer_device: cpu\n",
    "  offload_param_device: none\n",
    "  zero3_init_flag: false\n",
    "  zero3_save_16bit_model: false\n",
    "  zero_stage: 2\n",
    "downcast_bf16: 'no'\n",
    "machine_rank: 0\n",
    "main_training_function: main\n",
    "num_machines: 1\n",
    "num_processes: 1\n",
    "rdzv_backend: static\n",
    "same_network: true\n",
    "tpu_env: []\n",
    "tpu_use_cluster: false\n",
    "tpu_use_sudo: false\n",
    "use_cpu: false\n",
    "\"\"\".format(grad_acc=TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n",
    "\n",
    "# Save DeepSpeed config\n",
    "os.makedirs(\"train/config\", exist_ok=True)\n",
    "ds_config_path = \"train/config/colab_zero2.yaml\"\n",
    "with open(ds_config_path, 'w') as f:\n",
    "    f.write(ds_config_yaml)\n",
    "\n",
    "print(\"üöÄ Starting XiYan-SQL Training\")\n",
    "print(\"=\"*60)\n",
    "print(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\n",
    "print(f\"üìä Dataset: {TRAINING_CONFIG['data_path']} (English)\")\n",
    "print(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\n",
    "print(f\"üéØ Effective Batch: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\n",
    "print(f\"üìè Max Length: {TRAINING_CONFIG['max_length']} tokens\")\n",
    "print(f\"üîß LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\n‚è≥ Training will take several hours...\")\n",
    "print(\"üí° Keep this tab active to prevent disconnection\\n\")\n",
    "\n",
    "# Build training command - use absolute paths\n",
    "cmd = [\n",
    "    \"accelerate\", \"launch\",\n",
    "    \"--config_file\", ds_config_path,\n",
    "    \"--num_processes\", \"1\",\n",
    "    \"train/sft4xiyan.py\",\n",
    "    \"--save_only_model\", \"True\",\n",
    "    \"--resume\", \"False\",\n",
    "    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n",
    "    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n",
    "    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n",
    "    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n",
    "    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
    "    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n",
    "    \"--save_total_limit\", \"3\",\n",
    "    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
    "    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
    "    \"--adam_beta2\", \"0.95\",\n",
    "    \"--warmup_ratio\", \"0.1\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--report_to\", \"none\",\n",
    "    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n",
    "    \"--lazy_preprocess\", \"False\",\n",
    "    \"--gradient_checkpointing\", \"True\",\n",
    "    \"--predict_with_generate\", \"True\",\n",
    "    \"--include_inputs_for_metrics\", \"True\",\n",
    "    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n",
    "    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n",
    "    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n",
    "    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n",
    "    \"--torch_compile\", \"False\",\n",
    "    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n",
    "    \"--model_type\", \"auto\",\n",
    "    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n",
    "    \"--bf16\",\n",
    "    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n",
    "]\n",
    "\n",
    "# Show the full command for debugging\n",
    "print(\"üìù Training command:\")\n",
    "print(\" \".join(cmd))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# Run training\n",
    "try:\n",
    "    result = subprocess.run(cmd, cwd=TRAINING_DIR, check=False)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n",
    "        print(\"=\"*60)\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"‚ùå Training failed with return code {result.returncode}\")\n",
    "        print(\"=\"*60)\n",
    "        print(\"\\nüí° Common issues:\")\n",
    "        print(\"  - Dataset not found: Check that nl2sql_standard_train_en.json exists\")\n",
    "        print(\"  - Model not found: Check MODEL_PATH is correct\")\n",
    "        print(\"  - Out of memory: Try reducing max_length or batch_size\")\n",
    "        print(\"\\nüîç Check the error messages above for more details\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_training"
   },
   "source": [
    "## Step 8: Save Trained Model to Google Drive (Optional)\n",
    "\n",
    "After training completes, save your model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to trained model\n",
    "TRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Destination in Google Drive\n",
    "DRIVE_SAVE_PATH = f\"/content/drive/MyDrive/XiYan-SQL/Trained-Models/{TRAINING_CONFIG['expr_id']}\"\n",
    "\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    print(f\"üì• Copying trained model to Google Drive...\")\n",
    "    print(f\"   From: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   To: {DRIVE_SAVE_PATH}\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    # Copy model\n",
    "    if os.path.exists(DRIVE_SAVE_PATH):\n",
    "        shutil.rmtree(DRIVE_SAVE_PATH)\n",
    "    \n",
    "    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n",
    "    print(f\"\\n‚úÖ Model saved to Google Drive!\")\n",
    "    print(f\"üìÅ Location: {DRIVE_SAVE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n",
    "    print(\"Make sure training completed successfully in Step 7.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Merge LoRA Adapter with Base Model (Optional)\n",
    "\n",
    "Merge the trained LoRA adapter with the base model to create a single, deployable model. This step is **required** if you want to use the model without loading the adapter separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Paths\n",
    "BASE_MODEL_PATH = TRAINING_CONFIG[\"model_path\"]\n",
    "ADAPTER_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Find the latest checkpoint\n",
    "checkpoint_dirs = []\n",
    "if os.path.exists(ADAPTER_PATH):\n",
    "    for item in os.listdir(ADAPTER_PATH):\n",
    "        item_path = os.path.join(ADAPTER_PATH, item)\n",
    "        if os.path.isdir(item_path) and (\"checkpoint\" in item.lower() or \"adapter\" in item.lower()):\n",
    "            checkpoint_dirs.append(item_path)\n",
    "\n",
    "if checkpoint_dirs:\n",
    "    checkpoint_dirs.sort(key=lambda x: os.path.getmtime(x), reverse=True)\n",
    "    ADAPTER_CHECKPOINT = checkpoint_dirs[0]\n",
    "else:\n",
    "    ADAPTER_CHECKPOINT = ADAPTER_PATH\n",
    "\n",
    "# Output path for merged model\n",
    "MERGED_MODEL_PATH = f\"/content/drive/MyDrive/XiYan-SQL/Trained-Models/{TRAINING_CONFIG['expr_id']}-merged\"\n",
    "\n",
    "print(\"üîÑ Merging LoRA adapter with base model...\")\n",
    "print(f\"   Base model: {BASE_MODEL_PATH}\")\n",
    "print(f\"   Adapter: {ADAPTER_CHECKPOINT}\")\n",
    "print(f\"   Output: {MERGED_MODEL_PATH}\")\n",
    "print(\"\\n‚è≥ This may take a few minutes...\\n\")\n",
    "\n",
    "try:\n",
    "    # Load base model and tokenizer\n",
    "    print(\"üì• Loading base model...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        BASE_MODEL_PATH, \n",
    "        use_fast=False, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    \n",
    "    # Load LoRA adapter\n",
    "    print(\"üì• Loading LoRA adapter...\")\n",
    "    lora_model = PeftModel.from_pretrained(\n",
    "        base_model,\n",
    "        ADAPTER_CHECKPOINT,\n",
    "        torch_dtype=torch.float16,\n",
    "        device_map='auto'\n",
    "    )\n",
    "    \n",
    "    # Merge adapter into base model\n",
    "    print(\"üîó Merging adapter into base model...\")\n",
    "    merged_model = lora_model.merge_and_unload()\n",
    "    \n",
    "    # Save merged model\n",
    "    print(f\"üíæ Saving merged model to {MERGED_MODEL_PATH}...\")\n",
    "    os.makedirs(MERGED_MODEL_PATH, exist_ok=True)\n",
    "    merged_model.save_pretrained(MERGED_MODEL_PATH)\n",
    "    tokenizer.save_pretrained(MERGED_MODEL_PATH)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Merged model saved successfully!\")\n",
    "    print(f\"üìÅ Location: {MERGED_MODEL_PATH}\")\n",
    "    print(f\"\\nüí° You can now use this merged model directly without loading the adapter separately.\")\n",
    "    \n",
    "    # Save paths for quick testing\n",
    "    globals()['MERGED_MODEL_FOR_TESTING'] = MERGED_MODEL_PATH\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during merging: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"  - Make sure the base model path is correct\")\n",
    "    print(\"  - Verify the adapter checkpoint exists\")\n",
    "    print(\"  - Check that you have enough disk space in Google Drive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 10: Quick Inference Test\n",
    "\n",
    "Test your trained model with sample questions to verify it's generating reasonable SQL queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Use the same template format as training\n",
    "NL2SQLITE_TEMPLATE_EN = \"\"\"You are a SQLite expert. You need to read and understand the following„ÄêDatabase Schema„Äëdescription and the possible provided„ÄêEvidence„Äë, and use valid SQLite knowledge to generate SQL for answering the„ÄêQuestion„Äë.\n",
    "„ÄêQuestion„Äë\n",
    "{question}\n",
    "\n",
    "„ÄêDatabase Schema„Äë\n",
    "{db_schema}\n",
    "\n",
    "„ÄêEvidence„Äë\n",
    "{evidence}\n",
    "\n",
    "„ÄêQuestion„Äë\n",
    "{question}\n",
    "\n",
    "```sql\"\"\"\n",
    "\n",
    "\n",
    "def extract_sql_only(text):\n",
    "    \"\"\"Extract only SQL from model output, removing explanations.\"\"\"\n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    text = text.strip()\n",
    "    \n",
    "    # Pattern 1: SQL in markdown code blocks\n",
    "    if '```sql' in text:\n",
    "        parts = text.split('```sql')\n",
    "        if len(parts) > 1:\n",
    "            sql = parts[1].split('```')[0].strip()\n",
    "            return sql\n",
    "    \n",
    "    # Pattern 2: SQL in plain code blocks\n",
    "    if '```' in text:\n",
    "        parts = text.split('```')\n",
    "        if len(parts) > 1:\n",
    "            sql = parts[1].strip()\n",
    "            # Check if it looks like SQL\n",
    "            if any(sql.upper().startswith(kw) for kw in ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'ALTER', 'DROP', 'WITH']):\n",
    "                return sql\n",
    "    \n",
    "    # Pattern 3: SQL starts with SELECT/INSERT/etc\n",
    "    sql_keywords = ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'ALTER', 'DROP', 'WITH']\n",
    "    for keyword in sql_keywords:\n",
    "        if text.upper().startswith(keyword):\n",
    "            # Take until we hit explanation or end\n",
    "            lines = text.split('\\n')\n",
    "            sql_lines = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                # Stop at explanation patterns\n",
    "                explanation_indicators = [\n",
    "                    'this query', 'here\\'s', 'to find', 'you can use',\n",
    "                    'the query', 'this will', 'selects', 'groups',\n",
    "                    'This query', 'Here\\'s', 'To find', 'You can use',\n",
    "                    'The query', 'This will'\n",
    "                ]\n",
    "                # Check if line contains explanation (but not if it's actual SQL)\n",
    "                is_explanation = any(indicator in line.lower() for indicator in explanation_indicators)\n",
    "                # Also check for markdown code block endings that suggest explanation follows\n",
    "                if is_explanation or ('```' in line and 'SELECT' not in line.upper()):\n",
    "                    # Only break if we already have SQL\n",
    "                    if len(sql_lines) > 0:\n",
    "                        break\n",
    "                sql_lines.append(line)\n",
    "            return ' '.join(sql_lines)\n",
    "    \n",
    "    # Pattern 4: Look for SQL after explanation text\n",
    "    # Find first occurrence of SQL keywords\n",
    "    for keyword in sql_keywords:\n",
    "        idx = text.upper().find(keyword)\n",
    "        if idx != -1:\n",
    "            # Extract from that position\n",
    "            sql_text = text[idx:].strip()\n",
    "            lines = sql_text.split('\\n')\n",
    "            sql_lines = []\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                # Stop at new explanation\n",
    "                explanation_indicators = [\n",
    "                    'this query', 'here\\'s', 'to find', 'you can use',\n",
    "                    'the query', 'this will'\n",
    "                ]\n",
    "                if any(indicator in line.lower() for indicator in explanation_indicators):\n",
    "                    if len(sql_lines) > 0:\n",
    "                        break\n",
    "                sql_lines.append(line)\n",
    "            return ' '.join(sql_lines)\n",
    "    \n",
    "    return text\n",
    "\n",
    "\n",
    "# Determine which model to test\n",
    "# Priority: 1) Merged model, 2) Latest checkpoint with adapter, 3) Base model with adapter\n",
    "model_to_test = None\n",
    "use_adapter = False\n",
    "adapter_path = None\n",
    "\n",
    "if 'MERGED_MODEL_FOR_TESTING' in globals() and os.path.exists(globals()['MERGED_MODEL_FOR_TESTING']):\n",
    "    model_to_test = globals()['MERGED_MODEL_FOR_TESTING']\n",
    "    print(f\"üéØ Testing merged model: {model_to_test}\")\n",
    "elif 'ADAPTER_CHECKPOINT' in globals() and os.path.exists(globals()['ADAPTER_CHECKPOINT']):\n",
    "    model_to_test = TRAINING_CONFIG[\"model_path\"]\n",
    "    adapter_path = globals()['ADAPTER_CHECKPOINT']\n",
    "    use_adapter = True\n",
    "    print(f\"üéØ Testing base model + adapter:\")\n",
    "    print(f\"   Base: {model_to_test}\")\n",
    "    print(f\"   Adapter: {adapter_path}\")\n",
    "else:\n",
    "    print(\"‚ùå No trained model found!\")\n",
    "    print(\"   Please complete Step 7 (training) and optionally Step 9 (merging) first.\")\n",
    "    model_to_test = None\n",
    "\n",
    "if model_to_test:\n",
    "    print(\"\\nüì• Loading model and tokenizer...\")\n",
    "    print(\"   (This may take 1-2 minutes)\\n\")\n",
    "    \n",
    "    try:\n",
    "        # Load tokenizer\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_to_test,\n",
    "            use_fast=False,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        \n",
    "        # Load model\n",
    "        if use_adapter:\n",
    "            # Load base model first\n",
    "            base_model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_to_test,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            # Load adapter on top\n",
    "            model = PeftModel.from_pretrained(\n",
    "                base_model,\n",
    "                adapter_path,\n",
    "                torch_dtype=torch.bfloat16\n",
    "            )\n",
    "            model.eval()\n",
    "        else:\n",
    "            # Load merged model directly\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_to_test,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True\n",
    "            )\n",
    "            model.eval()\n",
    "        \n",
    "        print(\"‚úÖ Model loaded successfully!\\n\")\n",
    "        \n",
    "        # Test cases (using m-schema format like training data)\n",
    "        # All from movie_3 database\n",
    "        movie_3_schema = \"\"\"„ÄêDB_ID„Äë movie_3\n",
    "„ÄêSchema„Äë\n",
    "# Table: film\n",
    "[\n",
    "(film_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(title:TEXT, Examples: [ACADEMY DINOSAUR, ACE GOLDFINGER, ADAPTATION HOLES]),\n",
    "(description:TEXT),\n",
    "(release_year:TEXT, Examples: [2006]),\n",
    "(language_id:INTEGER, Examples: [1]),\n",
    "(original_language_id:INTEGER),\n",
    "(rental_duration:INTEGER, Examples: [6, 3, 7]),\n",
    "(rental_rate:REAL, Examples: [0.99, 4.99, 2.99]),\n",
    "(length:INTEGER, Examples: [86, 48, 50]),\n",
    "(replacement_cost:REAL, Examples: [20.99, 12.99, 18.99]),\n",
    "(rating:TEXT, Examples: [PG, G, NC-17]),\n",
    "(special_features:TEXT, Examples: [Trailers,Deleted Scenes, Commentaries,Behind the Scenes]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 05:03:42.0])\n",
    "]\n",
    "# Table: rental\n",
    "[\n",
    "(rental_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(rental_date:DATETIME, Examples: [2005-05-24 22:53:30.0]),\n",
    "(inventory_id:INTEGER, Examples: [367, 1525, 1711]),\n",
    "(customer_id:INTEGER, Examples: [130, 459, 408]),\n",
    "(return_date:DATETIME, Examples: [2005-05-26 22:04:30.0]),\n",
    "(staff_id:INTEGER, Examples: [1, 2]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 21:30:53.0])\n",
    "]\n",
    "# Table: store\n",
    "[\n",
    "(store_id:INTEGER, Primary Key, Examples: [1, 2]),\n",
    "(manager_staff_id:INTEGER, Examples: [1, 2]),\n",
    "(address_id:INTEGER, Examples: [1, 2]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:57:12.0])\n",
    "]\n",
    "# Table: inventory\n",
    "[\n",
    "(inventory_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(film_id:INTEGER, Examples: [1, 2, 3]),\n",
    "(store_id:INTEGER, Examples: [1, 2]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 05:09:17.0])\n",
    "]\n",
    "# Table: address\n",
    "[\n",
    "(address_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(address:TEXT, Examples: [47 MySakila Drive, 28 MySQL Boulevard, 23 Workhaven Lane]),\n",
    "(address2:TEXT),\n",
    "(district:TEXT, Examples: [Alberta, QLD, Nagasaki]),\n",
    "(city_id:INTEGER, Examples: [300, 576, 463]),\n",
    "(postal_code:TEXT, Examples: [35200, 17886, 83579]),\n",
    "(phone:TEXT, Examples: [14033335568, 6172235589, 28303384290]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:45:30.0])\n",
    "]\n",
    "# Table: country\n",
    "[\n",
    "(country_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(country:TEXT, Examples: [Afghanistan, Algeria, American Samoa]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:44:00.0])\n",
    "]\n",
    "# Table: city\n",
    "[\n",
    "(city_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(city:TEXT, Examples: [A Corua (La Corua), Abha, Abu Dhabi]),\n",
    "(country_id:INTEGER, Examples: [87, 82, 101]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:45:25.0])\n",
    "]\n",
    "# Table: film_actor\n",
    "[\n",
    "(actor_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(film_id:INTEGER, Primary Key, Examples: [1, 23, 25]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 05:05:03.0])\n",
    "]\n",
    "# Table: payment\n",
    "[\n",
    "(payment_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(customer_id:INTEGER, Examples: [1, 2, 3]),\n",
    "(staff_id:INTEGER, Examples: [1, 2]),\n",
    "(rental_id:INTEGER, Examples: [76, 573, 1185]),\n",
    "(amount:REAL, Examples: [2.99, 0.99, 5.99]),\n",
    "(payment_date:DATETIME, Examples: [2005-05-25 11:30:37.0]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 22:12:30.0])\n",
    "]\n",
    "# Table: film_text\n",
    "[\n",
    "(film_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(title:TEXT, Examples: [ACADEMY DINOSAUR, ACE GOLDFINGER, ADAPTATION HOLES]),\n",
    "(description:TEXT)\n",
    "]\n",
    "# Table: customer\n",
    "[\n",
    "(customer_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(store_id:INTEGER, Examples: [1, 2]),\n",
    "(first_name:TEXT, Examples: [MARY, PATRICIA, LINDA]),\n",
    "(last_name:TEXT, Examples: [SMITH, JOHNSON, WILLIAMS]),\n",
    "(email:TEXT),\n",
    "(address_id:INTEGER, Examples: [5, 6, 7]),\n",
    "(active:INTEGER, Examples: [1, 0]),\n",
    "(create_date:DATETIME, Examples: [2006-02-14 22:04:36.0]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:57:20.0])\n",
    "]\n",
    "# Table: staff\n",
    "[\n",
    "(staff_id:INTEGER, Primary Key, Examples: [1, 2]),\n",
    "(first_name:TEXT, Examples: [Mike, Jon]),\n",
    "(last_name:TEXT, Examples: [Hillyer, Stephens]),\n",
    "(address_id:INTEGER, Examples: [3, 4]),\n",
    "(picture:BLOB),\n",
    "(email:TEXT),\n",
    "(store_id:INTEGER, Examples: [1, 2]),\n",
    "(active:INTEGER, Examples: [1]),\n",
    "(username:TEXT, Examples: [Mike, Jon]),\n",
    "(password:TEXT),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:57:16.0])\n",
    "]\n",
    "# Table: language\n",
    "[\n",
    "(language_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(name:TEXT, Examples: [English, Italian, Japanese]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 05:02:19.0])\n",
    "]\n",
    "# Table: film_category\n",
    "[\n",
    "(film_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(category_id:INTEGER, Primary Key, Examples: [6, 11, 8]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 05:07:09.0])\n",
    "]\n",
    "# Table: category\n",
    "[\n",
    "(category_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(name:TEXT, Examples: [Action, Animation, Children]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:46:27.0])\n",
    "]\n",
    "# Table: actor\n",
    "[\n",
    "(actor_id:INTEGER, Primary Key, Examples: [1, 2, 3]),\n",
    "(first_name:TEXT, Examples: [PENELOPE, NICK, ED]),\n",
    "(last_name:TEXT, Examples: [GUINESS, WAHLBERG, CHASE]),\n",
    "(last_update:DATETIME, Examples: [2006-02-15 04:34:33.0])\n",
    "]\n",
    "„ÄêForeign keys„Äë\n",
    "film.original_language_id=language.language_id\n",
    "film.language_id=language.language_id\n",
    "rental.staff_id=staff.staff_id\n",
    "rental.customer_id=customer.customer_id\n",
    "rental.inventory_id=inventory.inventory_id\n",
    "store.address_id=address.address_id\n",
    "store.manager_staff_id=staff.staff_id\n",
    "inventory.store_id=store.store_id\n",
    "inventory.film_id=film.film_id\n",
    "address.city_id=city.city_id\n",
    "city.country_id=country.country_id\n",
    "film_actor.film_id=film.film_id\n",
    "film_actor.actor_id=actor.actor_id\n",
    "payment.rental_id=rental.rental_id\n",
    "payment.staff_id=staff.staff_id\n",
    "payment.customer_id=customer.customer_id\n",
    "customer.address_id=address.address_id\n",
    "customer.store_id=store.store_id\n",
    "staff.store_id=store.store_id\n",
    "staff.address_id=address.address_id\n",
    "film_category.category_id=category.category_id\n",
    "film_category.film_id=film.film_id\"\"\"\n",
    "\n",
    "        test_cases = [\n",
    "            {\n",
    "                \"schema\": movie_3_schema,\n",
    "                \"question\": \"Among the times Mary Smith had rented a movie, how many of them happened in June, 2005?\",\n",
    "                \"evidence\": \"in June 2005 refers to year(payment_date) = 2005 and month(payment_date) = 6\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": movie_3_schema,\n",
    "                \"question\": \"Please give the full name of the customer who had made the biggest amount of payment in one single film rental.\",\n",
    "                \"evidence\": \"full name refers to first_name, last_name; the biggest amount refers to max(amount)\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": movie_3_schema,\n",
    "                \"question\": \"How much in total had the customers in Italy spent on film rentals?\",\n",
    "                \"evidence\": \"total = sum(amount); Italy refers to country = 'Italy'\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": movie_3_schema,\n",
    "                \"question\": \"Among the payments made by Mary Smith, how many of them are over 4.99?\",\n",
    "                \"evidence\": \"over 4.99 refers to amount > 4.99\"\n",
    "            },\n",
    "            {\n",
    "                \"schema\": movie_3_schema,\n",
    "                \"question\": \"What is the average amount of money spent by a customer in Italy on a single film rental?\",\n",
    "                \"evidence\": \"Italy refers to country = 'Italy'; average amount = divide(sum(amount), count(customer_id)) where country = 'Italy'\"\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        print(\"=\"*80)\n",
    "        print(\"üß™ QUICK INFERENCE TEST\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for i, test_case in enumerate(test_cases, 1):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Test Case {i}:\")\n",
    "            print(f\"{'='*80}\")\n",
    "            print(f\"\\nüìù Question: {test_case['question']}\")\n",
    "            print(f\"\\nüìä Schema:\")\n",
    "            for line in test_case['schema'].split('\\n')[:5]:  # Show first 5 lines\n",
    "                print(f\"   {line}\")\n",
    "            print(\"   ...\")\n",
    "            \n",
    "            # Build prompt using the same template as training\n",
    "            prompt_text = NL2SQLITE_TEMPLATE_EN.format(\n",
    "                question=test_case['question'],\n",
    "                db_schema=test_case['schema'],\n",
    "                evidence=test_case['evidence']\n",
    "            )\n",
    "            \n",
    "            # Create conversation format (same as training data)\n",
    "            conversations = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt_text\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template (same as sql_infer.py)\n",
    "            text = tokenizer.apply_chat_template(\n",
    "                conversations,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=4096)\n",
    "            inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # Generate (using similar params as sql_infer.py)\n",
    "            print(\"\\n‚è≥ Generating SQL...\")\n",
    "            with torch.no_grad():\n",
    "                outputs = model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=256,\n",
    "                    do_sample=False,\n",
    "                    pad_token_id=tokenizer.pad_token_id,\n",
    "                    eos_token_id=tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # Extract only the generated part (after input)\n",
    "            generated_ids = outputs[0][len(inputs['input_ids'][0]):]\n",
    "            raw_output = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            # Extract only SQL, removing any explanations\n",
    "            sql_output = extract_sql_only(raw_output)\n",
    "            \n",
    "            print(f\"\\n‚úÖ Generated SQL:\")\n",
    "            print(\"‚îÄ\"*80)\n",
    "            print(sql_output)\n",
    "            print(\"‚îÄ\"*80)\n",
    "            \n",
    "            # Show raw output if it differs significantly (for debugging)\n",
    "            if raw_output != sql_output and len(raw_output) > len(sql_output) + 20:\n",
    "                print(f\"\\n‚ö†Ô∏è  Note: Model also generated explanatory text (removed)\")\n",
    "                print(f\"   Raw output length: {len(raw_output)} chars, SQL length: {len(sql_output)} chars\")\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(\"‚úÖ Quick inference test completed!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        # Clean up to free memory\n",
    "        print(\"\\nüßπ Cleaning up memory...\")\n",
    "        if 'model' in locals():\n",
    "            del model\n",
    "        if 'tokenizer' in locals():\n",
    "            del tokenizer\n",
    "        if 'base_model' in locals():\n",
    "            del base_model\n",
    "        torch.cuda.empty_cache()\n",
    "        print(\"‚úÖ Memory freed\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error during inference: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Errors\n",
    "- Reduce `batch_size` to 1\n",
    "- Reduce `max_length` to 8192 or 4096\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- The DeepSpeed config already uses CPU offloading, which helps\n",
    "\n",
    "### Model Not Found\n",
    "- Check that `MODEL_DRIVE_PATH` in Step 4 is correct\n",
    "- Verify the model folder exists in Google Drive\n",
    "- Ensure the model folder contains all required files (config.json, tokenizer files, etc.)\n",
    "\n",
    "### Dataset Not Found\n",
    "- Check that dataset paths in Step 5 are correct\n",
    "- Verify files exist in Google Drive\n",
    "- If processing raw data, ensure `db_conn.json` exists\n",
    "\n",
    "### Training Too Slow\n",
    "- Colab free tier has limited GPU time\n",
    "- Consider using Colab Pro for longer training sessions\n",
    "- Reduce dataset size for testing (set `sample_num` in dataset config)\n",
    "\n",
    "### Connection Issues\n",
    "- Colab sessions may disconnect after inactivity\n",
    "- Use `nohup` or save checkpoints frequently\n",
    "- Consider running training in multiple sessions if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}