{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# XiYan-SQL Training on Google Colab\n",
    "\n",
    "This notebook provides a complete step-by-step guide to train the XiYan-SQL model on Google Colab.\n",
    "\n",
    "## Prerequisites\n",
    "- Upload your model files to Google Drive (e.g., `Qwen2.5-Coder-3B-Instruct` folder)\n",
    "- Upload your dataset files to Google Drive (raw data, processed data, or both)\n",
    "- Enable GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required packages for XiYan-SQL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libaio-dev  # Required for DeepSpeed\n",
    "\n",
    "print(\"üì¶ Installing Python packages...\")\n",
    "print(\"‚ö†Ô∏è  Note: Installing in specific order to avoid numpy/DeepSpeed conflicts.\\n\")\n",
    "\n",
    "# CRITICAL: Install numpy FIRST before anything else\n",
    "!pip install -q --disable-pip-version-check \"numpy>=1.23.0,<2.0\"\n",
    "\n",
    "# Install PyTorch with specific numpy-compatible version\n",
    "!pip install -q --disable-pip-version-check \"torch==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -q --disable-pip-version-check \"torchaudio==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -q --disable-pip-version-check \"torchvision==0.24.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "\n",
    "# Install transformers and related packages\n",
    "!pip install -q --disable-pip-version-check transformers==4.42.3\n",
    "!pip install -q --disable-pip-version-check accelerate>=1.12.0\n",
    "!pip install -q --disable-pip-version-check peft==0.11.1\n",
    "!pip install -q --disable-pip-version-check datasets>=3.0.0\n",
    "\n",
    "# Install DeepSpeed AFTER numpy and torch are set\n",
    "print(\"\\nüîß Installing DeepSpeed (may show some warnings)...\")\n",
    "!pip install -q --disable-pip-version-check --no-cache-dir deepspeed>=0.18.4\n",
    "\n",
    "# Install remaining packages\n",
    "!pip install -q --disable-pip-version-check llama-index>=0.9.6.post2\n",
    "!pip install -q --disable-pip-version-check markupsafe==2.1.3\n",
    "!pip install -q --disable-pip-version-check modelscope>=1.33.0\n",
    "!pip install -q --disable-pip-version-check mysql-connector-python>=9.5.0\n",
    "!pip install -q --disable-pip-version-check ninja>=1.13.0\n",
    "!pip install -q --disable-pip-version-check packaging>=24.1\n",
    "!pip install -q --disable-pip-version-check pandas>=2.3.3\n",
    "!pip install -q --disable-pip-version-check \"protobuf>=6.33.3\"\n",
    "!pip install -q --disable-pip-version-check psycopg2-binary>=2.9.11\n",
    "!pip install -q --disable-pip-version-check sentencepiece>=0.2.1\n",
    "!pip install -q --disable-pip-version-check setuptools>=70.2.0\n",
    "!pip install -q --disable-pip-version-check sqlalchemy>=2.0.45\n",
    "!pip install -q --disable-pip-version-check sqlglot>=28.5.0\n",
    "!pip install -q --disable-pip-version-check swanlab>=0.7.6\n",
    "!pip install -q --disable-pip-version-check textdistance>=4.6.3\n",
    "!pip install -q --disable-pip-version-check wheel>=0.45.1\n",
    "!pip install -q --disable-pip-version-check jedi>=0.16\n",
    "\n",
    "# Install flash-attn (optional, for faster attention)\n",
    "print(\"\\nüî® Attempting to install flash-attn (this may take a few minutes)...\")\n",
    "import subprocess\n",
    "result = subprocess.run(\n",
    "    [\"pip\", \"install\", \"-q\", \"--no-build-isolation\", \"flash-attn\"],\n",
    "    capture_output=True,\n",
    "    text=True\n",
    ")\n",
    "if result.returncode == 0:\n",
    "    print(\"‚úÖ flash-attn installed successfully\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  flash-attn installation failed (this is optional, continuing without it)\")\n",
    "\n",
    "print(\"\\n‚úÖ Core dependencies installed!\")\n",
    "print(\"\\nüí° If you see numpy warnings, they are expected and won't affect training.\")\n",
    "\n",
    "# Verify installation\n",
    "print(\"\\nüîç Verifying installation...\")\n",
    "try:\n",
    "    import torch\n",
    "    import transformers\n",
    "    import accelerate\n",
    "    import deepspeed\n",
    "    import peft\n",
    "    import numpy as np\n",
    "\n",
    "    print(f\"‚úÖ PyTorch: {torch.__version__}\")\n",
    "    print(f\"‚úÖ Transformers: {transformers.__version__}\")\n",
    "    print(f\"‚úÖ Accelerate: {accelerate.__version__}\")\n",
    "    print(f\"‚úÖ DeepSpeed: {deepspeed.__version__}\")\n",
    "    print(f\"‚úÖ PEFT: {peft.__version__}\")\n",
    "    print(f\"‚úÖ NumPy: {np.__version__}\")\n",
    "    print(f\"‚úÖ CUDA Available: {torch.cuda.is_available()}\")\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"‚úÖ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        gpu_mem = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "        print(f\"‚úÖ GPU Memory: {gpu_mem:.1f} GB\")\n",
    "        \n",
    "        if gpu_mem >= 14:\n",
    "            print(\"\\nüéØ Your GPU has 15GB+ memory - perfect for optimized training!\")\n",
    "        elif gpu_mem >= 10:\n",
    "            print(\"\\nüìä Your GPU has 12GB memory - good for moderate training.\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è  Your GPU has limited memory - training will use conservative settings.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå No GPU detected! Make sure to enable GPU in Runtime ‚Üí Change runtime type\")\n",
    "\n",
    "    print(\"\\nüöÄ Ready to proceed!\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå Import error: {e}\")\n",
    "    print(\"\\nüîÑ If you see numpy errors, restart runtime and run this cell again.\")\n",
    "    print(\"   Go to: Runtime ‚Üí Restart runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "import swanlab\nimport os\nfrom getpass import getpass\n\n# Option 1: Interactive login (recommended for first time)\nprint(\"üîê SwanLab Login\")\nprint(\"=\" * 60)\nprint(\"SwanLab will track your training metrics automatically.\")\nprint(\"Visit https://swanlab.cn to get your API key.\")\nprint(\"=\" * 60)\n\n# Check if already logged in\ntry:\n    # Try to initialize without login to check if already authenticated\n    test_run = swanlab.init(mode=\"disabled\")\n    test_run.finish()\n    print(\"\\n‚úÖ Already logged in to SwanLab!\")\nexcept:\n    print(\"\\nüìù Login options:\")\n    print(\"1. Interactive login (paste API key)\")\n    print(\"2. Skip for now (you can login later)\")\n    \n    choice = input(\"\\nEnter choice (1 or 2): \").strip()\n    \n    if choice == \"1\":\n        api_key = getpass(\"Enter your SwanLab API key: \")\n        if api_key:\n            os.environ['SWANLAB_API_KEY'] = api_key\n            swanlab.login(api_key=api_key)\n            print(\"\\n‚úÖ Logged in to SwanLab successfully!\")\n            print(\"Your training will be tracked at: https://swanlab.cn\")\n        else:\n            print(\"\\n‚ö†Ô∏è  No API key provided. Training will proceed without SwanLab tracking.\")\n    else:\n        print(\"\\n‚ö†Ô∏è  Skipped SwanLab login. Training will proceed without tracking.\")\n        print(\"üí° You can login anytime by running: swanlab.login(api_key='your_key')\")\n\nprint(\"\\n‚úÖ Ready to proceed!\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Step 1.5: Login to SwanLab (Optional but Recommended)\n\nSwanLab will automatically track your training metrics, including:\n- Training loss\n- Learning rate\n- GPU/CPU/RAM usage\n- All hyperparameters\n- Model checkpoints\n\n**Get your API key:** https://swanlab.cn (sign up/login, then go to Settings ‚Üí API Key)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Clone Repository\n",
    "\n",
    "Clone the XiYan-SQL repository to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Change to content directory\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone the repository\n",
    "# Replace with your repository URL\n",
    "REPO_URL = \"https://github.com/rezaarrazi/XiYan-SQL.git\"  # ‚ö†Ô∏è UPDATE THIS\n",
    "\n",
    "if not os.path.exists('XiYan-SQL'):\n",
    "    os.system(f'git clone {REPO_URL}')\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Navigate to training directory\n",
    "os.chdir('XiYan-SQL/XiYan-SQLTraining')\n",
    "\n",
    "# Add to Python path so imports work correctly\n",
    "TRAINING_DIR = os.getcwd()\n",
    "if TRAINING_DIR not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_DIR)\n",
    "if os.path.dirname(TRAINING_DIR) not in sys.path:\n",
    "    sys.path.insert(0, os.path.dirname(TRAINING_DIR))\n",
    "\n",
    "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access model and dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully\")\n",
    "print(\"\\nüìÇ Drive path: /content/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Copy Model from Google Drive\n",
    "\n",
    "Copy your pre-downloaded model from Google Drive to the model directory.\n",
    "\n",
    "**Configured Path:** `My Drive/Xiyan-SQL/Models/Qwen/`\n",
    "\n",
    "The script will automatically detect and copy the model folder(s) from this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_model"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to your model in Google Drive\n",
    "MODEL_DRIVE_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Models/Qwen\"\n",
    "\n",
    "# Target directory in the repository\n",
    "MODEL_TARGET_DIR = \"train/model/Qwen\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(MODEL_TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model directory exists in Drive\n",
    "if os.path.exists(MODEL_DRIVE_PATH):\n",
    "    print(f\"üì• Found model directory at {MODEL_DRIVE_PATH}\")\n",
    "    \n",
    "    # List contents to see what's inside\n",
    "    contents = os.listdir(MODEL_DRIVE_PATH)\n",
    "    print(f\"üìÅ Contents: {contents}\")\n",
    "    \n",
    "    # Check if it's a single model folder or contains multiple model folders\n",
    "    model_folders = [item for item in contents if os.path.isdir(os.path.join(MODEL_DRIVE_PATH, item))]\n",
    "    \n",
    "    if len(model_folders) == 1:\n",
    "        # Single model folder - copy it directly\n",
    "        model_name = model_folders[0]\n",
    "        source_path = os.path.join(MODEL_DRIVE_PATH, model_name)\n",
    "        target_path = os.path.join(MODEL_TARGET_DIR, model_name)\n",
    "        \n",
    "        if os.path.exists(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying model '{model_name}' from {source_path}...\")\n",
    "            shutil.copytree(source_path, target_path)\n",
    "            print(f\"‚úÖ Model copied to {target_path}\")\n",
    "        \n",
    "        MODEL_PATH = target_path\n",
    "    else:\n",
    "        # Multiple folders or files - copy the entire Qwen directory\n",
    "        target_path = MODEL_TARGET_DIR\n",
    "        if os.path.exists(target_path) and os.listdir(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model directory already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying all models from {MODEL_DRIVE_PATH}...\")\n",
    "            for item in contents:\n",
    "                source_item = os.path.join(MODEL_DRIVE_PATH, item)\n",
    "                target_item = os.path.join(target_path, item)\n",
    "                if os.path.isdir(source_item):\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copytree(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "                else:\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copy2(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "            print(f\"‚úÖ All models copied to {target_path}\")\n",
    "        \n",
    "        # Set MODEL_PATH to the first model folder found, or let user specify\n",
    "        if model_folders:\n",
    "            MODEL_PATH = os.path.join(MODEL_TARGET_DIR, model_folders[0])\n",
    "            print(f\"\\nüìå Using model: {MODEL_PATH}\")\n",
    "            print(f\"üí° If you want to use a different model, update MODEL_PATH in Step 7\")\n",
    "        else:\n",
    "            MODEL_PATH = MODEL_TARGET_DIR\n",
    "            print(f\"\\nüìå Model directory: {MODEL_PATH}\")\n",
    "            print(f\"üí° Please specify the exact model folder name in Step 7\")\n",
    "    \n",
    "    print(f\"\\nüìå Model path for training: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {MODEL_DRIVE_PATH}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. Google Drive is mounted correctly\")\n",
    "    print(\"2. The path 'My Drive/Xiyan-SQL/Models/Qwen/' exists in your Drive\")\n",
    "    MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": [
    "## Step 5: Verify Training Dataset\n",
    "\n",
    "The English training dataset should already be in the repository (via Git LFS).\n",
    "\n",
    "**Expected file:** `train/datasets/nl2sql_standard_train_en.json` (55MB)\n",
    "\n",
    "If the file is not present, you can download it from Google Drive as a backup."
   ]
  },
  {
   "cell_type": "code",
   "source": "import os\nimport json\n\n# Check if training dataset exists in repository\nTRAIN_DATASET_PATH = \"train/datasets/nl2sql_standard_train_en.json\"\n\nif os.path.exists(TRAIN_DATASET_PATH):\n    print(f\"‚úÖ Training dataset found in repository!\")\n    print(f\"   Path: {TRAIN_DATASET_PATH}\")\n    \n    size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n    print(f\"   Size: {size_mb:.1f} MB\")\n    \n    # Quick verification\n    with open(TRAIN_DATASET_PATH, 'r') as f:\n        data = json.load(f)\n        print(f\"   Samples: {len(data)}\")\n        \n        # Check if English\n        if data and data[0].get('conversations'):\n            prompt = data[0]['conversations'][0]['content']\n            if prompt.startswith(\"You are a SQLite expert\"):\n                print(f\"   Language: ‚úÖ English\")\n            else:\n                print(f\"   Language: ‚ö†Ô∏è Not English\")\n    \n    print(\"\\nüéâ Ready to start training! Skip to Step 6.\")\n    \nelse:\n    print(f\"‚ö†Ô∏è  Training dataset not found in repository\")\n    print(f\"   Expected: {TRAIN_DATASET_PATH}\")\n    print(\"\\nüì• Attempting to download from Google Drive as backup...\")\n    \n    # Backup: Download from Google Drive\n    DRIVE_DATASET_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Dataset/nl2sql_standard_train_en.json\"\n    \n    if os.path.exists(DRIVE_DATASET_PATH):\n        import shutil\n        os.makedirs(\"train/datasets\", exist_ok=True)\n        shutil.copy2(DRIVE_DATASET_PATH, TRAIN_DATASET_PATH)\n        print(f\"‚úÖ Dataset copied from Google Drive\")\n        \n        size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n        print(f\"   Size: {size_mb:.1f} MB\")\n    else:\n        print(f\"‚ùå Dataset not found in Google Drive either\")\n        print(f\"   Expected: {DRIVE_DATASET_PATH}\")\n        print(\"\\nüí° Options:\")\n        print(\"1. Make sure Git LFS pulled the dataset when cloning\")\n        print(\"2. Upload nl2sql_standard_train_en.json to Google Drive\")\n        print(\"3. Or run data processing from BIRD raw data\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": "## Step 6: Configure Training Parameters\n\n**Auto-detects your GPU and optimizes settings:**\n- A100/H100 40GB+: High-end config (16K context, batch 6-8, LoRA rank 256)\n- L4 24GB: High performance (12K context, batch 3, LoRA rank 128)  \n- T4 15GB: Balanced config (4K context, batch 1, LoRA rank 64)\n\nFlash Attention is automatically enabled for Ampere+ GPUs (A100, H100, L4) and disabled for T4."
  },
  {
   "cell_type": "code",
   "source": "# Check available GPU memory\nimport subprocess\nimport re\nimport os\n\n# Set CUDA memory configuration to reduce fragmentation\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\ntry:\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], \n                          capture_output=True, text=True)\n    gpu_memory_mb = int(result.stdout.strip())\n    gpu_memory_gb = gpu_memory_mb / 1024\n    print(f\"üéÆ Detected GPU Memory: {gpu_memory_gb:.1f} GB\")\n    \n    # Detect GPU architecture for flash attention compatibility\n    gpu_name_result = subprocess.run(['nvidia-smi', '--query-gpu=name', '--format=csv,noheader'], \n                                     capture_output=True, text=True)\n    gpu_name = gpu_name_result.stdout.strip()\n    print(f\"üéÆ GPU: {gpu_name}\")\n    \n    # Flash attention only works on Ampere or newer (A100, A10, RTX 3090, RTX 4090, etc.)\n    # T4 is Turing architecture - doesn't support flash attention\n    supports_flash = any(x in gpu_name.upper() for x in ['A100', 'A10', 'RTX 30', 'RTX 40', 'H100', 'L4'])\n    if not supports_flash and 'T4' in gpu_name:\n        print(f\"‚ö†Ô∏è  T4 GPU detected - Flash Attention will be disabled (T4 is Turing, needs Ampere+)\")\n    elif supports_flash:\n        print(f\"‚úÖ Flash Attention supported on this GPU!\")\nexcept:\n    gpu_memory_gb = 15.0  # Default assumption\n    supports_flash = False\n    print(f\"‚ö†Ô∏è  Could not detect GPU, assuming {gpu_memory_gb} GB\")\n    print(f\"‚ö†Ô∏è  Flash Attention will be disabled for compatibility\")\n\n# Auto-configure based on GPU memory - VERY CONSERVATIVE to avoid OOM\nif gpu_memory_gb >= 70:\n    # 80GB GPU (H100 80GB) - Ultra conservative (model alone uses 55GB!)\n    MAX_LENGTH = 8192      # Further reduced from 12288\n    LORA_R = 64            # Further reduced from 128\n    BATCH_SIZE = 2         # Further reduced from 4\n    GRAD_ACC = 16          # Effective batch = 32\n    print(f\"üî• Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU (H100 80GB)\")\n    print(f\"   ‚ö†Ô∏è  Model uses ~55GB, being very conservative with remaining memory\")\nelif gpu_memory_gb >= 35:\n    # 40GB GPU (A100 40GB, H100 40GB) - Very conservative\n    MAX_LENGTH = 6144      # Further reduced from 8192\n    LORA_R = 64            # Further reduced from 128\n    BATCH_SIZE = 1         # Reduced from 2\n    GRAD_ACC = 32          # Effective batch = 32\n    print(f\"üöÄ Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU (A100/H100)\")\n    print(f\"   ‚ö†Ô∏è  Very conservative settings to avoid OOM\")\nelif gpu_memory_gb >= 22:\n    # 24GB GPU (L4, RTX 4090, A10) - Very conservative\n    MAX_LENGTH = 6144      # Reduced from 8192\n    LORA_R = 32            # Reduced from 64\n    BATCH_SIZE = 1         # Reduced from 2\n    GRAD_ACC = 32          # Effective batch = 32\n    print(f\"üéØ Using CONSERVATIVE config for {gpu_memory_gb:.1f}GB GPU\")\n    print(f\"   ‚ö†Ô∏è  Very conservative for stability\")\nelif gpu_memory_gb >= 14:\n    # 15GB GPU (T4, P100) - BALANCED\n    MAX_LENGTH = 4096      # Standard context\n    LORA_R = 64            # Balanced LoRA\n    BATCH_SIZE = 1         # Small batch\n    GRAD_ACC = 32          # Effective batch = 32\n    print(f\"üìä Using BALANCED config for {gpu_memory_gb:.1f}GB GPU\")\n    print(f\"   (Conservative settings to avoid OOM)\")\nelif gpu_memory_gb >= 10:\n    # 12GB GPU - LOW MEMORY\n    MAX_LENGTH = 2048\n    LORA_R = 32\n    BATCH_SIZE = 1\n    GRAD_ACC = 64\n    print(f\"üìä Using LOW MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\nelse:\n    # 8GB GPU - ULTRA-LOW\n    MAX_LENGTH = 1024\n    LORA_R = 16\n    BATCH_SIZE = 1\n    GRAD_ACC = 128\n    print(f\"üìä Using ULTRA-LOW MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\n\nTRAINING_CONFIG = {\n    # Experiment ID\n    \"expr_id\": \"nl2sql_3b_colab_en\",\n    \n    # Model path (set in Step 4)\n    \"model_path\": MODEL_PATH if 'MODEL_PATH' in globals() else \"train/model/Qwen/Qwen2.5-Coder-3B-Instruct\",\n    \n    # Dataset path - Using English version\n    \"data_path\": \"train/datasets/nl2sql_standard_train_en.json\",\n    \n    # Output directory\n    \"output_dir\": \"train/output/dense/nl2sql_3b_colab_en/\",\n    \n    # Training hyperparameters\n    \"epochs\": 5 if gpu_memory_gb >= 35 else 3,  # More epochs for A100/H100\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.1,\n    \"max_length\": MAX_LENGTH,\n    \n    # LoRA configuration\n    \"use_lora\": True,\n    \"lora_r\": LORA_R,\n    \"lora_alpha\": LORA_R * 2,\n    \n    # Batch configuration\n    \"batch_size\": BATCH_SIZE,\n    \"gradient_accumulation_steps\": GRAD_ACC,\n    \n    # Other settings\n    \"save_steps\": 200,\n    \"group_by_length\": True,\n    \"shuffle\": True,\n    \"use_flash_attention\": supports_flash,  # Auto-detect based on GPU\n    \"bf16\": True,\n}\n\nprint(\"\\nüìã Training Configuration:\")\nprint(f\"  Experiment ID: {TRAINING_CONFIG['expr_id']}\")\nprint(f\"  Dataset: {TRAINING_CONFIG['data_path']}\")\nprint(f\"  Max Length: {TRAINING_CONFIG['max_length']} tokens\")\nprint(f\"  LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\nprint(f\"  Batch Size: {TRAINING_CONFIG['batch_size']}\")\nprint(f\"  Gradient Accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"  Effective Batch Size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\nprint(f\"  Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\nprint(f\"  Flash Attention: {'‚úÖ Enabled' if TRAINING_CONFIG['use_flash_attention'] else '‚ùå Disabled (GPU not compatible)'}\")\n\nprint(\"\\nüí° Estimated Training Time:\")\nsamples = 9431\nsteps_per_epoch = samples // (TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps'])\ntotal_steps = steps_per_epoch * TRAINING_CONFIG['epochs']\n\n# Estimate time based on GPU and configuration\nif gpu_memory_gb >= 70:\n    time_per_step_sec = 2.5  # H100 80GB - slower due to conservative settings\nelif gpu_memory_gb >= 35:\n    time_per_step_sec = 3.0  # A100/H100 40GB\nelif gpu_memory_gb >= 22:\n    time_per_step_sec = 3.5  # L4/A10\nelif supports_flash:\n    time_per_step_sec = 3.5  # Other GPUs with flash attention\nelse:\n    time_per_step_sec = 4    # Without flash attention\n\ntotal_hours = (total_steps * time_per_step_sec) / 3600\nprint(f\"  Steps per epoch: ~{steps_per_epoch}\")\nprint(f\"  Total steps: ~{total_steps}\")\nprint(f\"  Estimated time: ~{total_hours:.1f} hours\")\n\nif gpu_memory_gb >= 70:\n    print(f\"\\n‚ö†Ô∏è  H100 80GB Note:\")\n    print(f\"   Base model uses ~55GB alone!\")\n    print(f\"   Using very conservative settings: 8K context, batch 2, LoRA 64\")\n    print(f\"   Training will work but slower than expected\")\nelif gpu_memory_gb >= 35:\n    print(f\"\\n‚ö†Ô∏è  A100/H100 40GB:\")\n    print(f\"   Using very conservative settings to ensure stability\")\n\nprint(\"\\n‚ö†Ô∏è  Colab Tips:\")\nif gpu_memory_gb >= 35:\n    print(\"  - Colab Pro+: 24 hour runtime limit with A100/H100\")\n    print(\"  - Training should complete well within time limit\")\nelse:\n    print(\"  - Colab Pro: Longer runtime than free tier\")\nprint(\"  - Keep browser tab active to prevent disconnection\")\nprint(\"  - Enable background execution in Colab settings\")\n\nprint(\"\\nüíæ Expected Memory Usage:\")\nif gpu_memory_gb >= 70:\n    print(f\"  ‚ö†Ô∏è  WARNING: Base model alone uses ~55GB!\")\n    print(f\"  - Ultra conservative: 8K context, batch 2, LoRA 64\")\n    print(f\"  - Expected total usage: ~65-70GB\")\n    print(f\"  - This leaves minimal headroom - training will be slow\")\nelif gpu_memory_gb >= 35:\n    print(f\"  - Very conservative: 6K context, batch 1, LoRA 64\")\n    print(f\"  - Expected usage: ~28-33GB\")\nelif gpu_memory_gb >= 22:\n    print(f\"  - Conservative: 6K context, batch 1, LoRA 32\")\n    print(f\"  - Expected usage: ~15-18GB\")\nelse:\n    print(f\"  - Conservative settings to prevent OOM\")\n    print(f\"  - Expected usage: ~6-12GB\")\n\nprint(\"\\nüîß Memory optimization: PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True\")\n\nif gpu_memory_gb >= 70:\n    print(\"\\nüí° Tip: The 3B model is quite large in memory.\")\n    print(\"   Consider using smaller batch sizes and shorter context if OOM persists.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "assemble_data"
   },
   "source": [
    "## Step 7: Start Training\n",
    "\n",
    "Run the training with your optimized configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport subprocess\nimport json\n\n# Set training directory\nTRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\nos.chdir(TRAINING_DIR)\n\n# Create DeepSpeed config YAML for single GPU (compatible with accelerate)\n# IMPORTANT: For 15GB GPU, we DON'T offload parameters to CPU - keep model on GPU!\n# Only offload optimizer states to save GPU memory\nds_config_yaml = \"\"\"compute_environment: LOCAL_MACHINE\ndistributed_type: DEEPSPEED\ndeepspeed_config:\n  gradient_accumulation_steps: {grad_acc}\n  gradient_clipping: 1.0\n  offload_optimizer_device: cpu\n  offload_param_device: none\n  zero3_init_flag: false\n  zero3_save_16bit_model: false\n  zero_stage: 2\ndowncast_bf16: 'no'\nmachine_rank: 0\nmain_training_function: main\nnum_machines: 1\nnum_processes: 1\nrdzv_backend: static\nsame_network: true\ntpu_env: []\ntpu_use_cluster: false\ntpu_use_sudo: false\nuse_cpu: false\n\"\"\".format(grad_acc=TRAINING_CONFIG[\"gradient_accumulation_steps\"])\n\n# Save DeepSpeed config\nos.makedirs(\"train/config\", exist_ok=True)\nds_config_path = \"train/config/colab_zero2.yaml\"\nwith open(ds_config_path, 'w') as f:\n    f.write(ds_config_yaml)\n\nprint(\"üöÄ Starting XiYan-SQL Training\")\nprint(\"=\"*60)\nprint(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\nprint(f\"üìä Dataset: {TRAINING_CONFIG['data_path']} (English)\")\nprint(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\nprint(f\"üéØ Effective Batch: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"üìè Max Length: {TRAINING_CONFIG['max_length']} tokens\")\nprint(f\"üîß LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\nprint(\"=\"*60)\nprint(\"\\n‚è≥ Training will take several hours...\")\nprint(\"üí° Keep this tab active to prevent disconnection\\n\")\n\n# Build training command - use absolute paths\ncmd = [\n    \"accelerate\", \"launch\",\n    \"--config_file\", ds_config_path,\n    \"--num_processes\", \"1\",\n    \"train/sft4xiyan.py\",\n    \"--save_only_model\", \"True\",\n    \"--resume\", \"False\",\n    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n    \"--save_strategy\", \"steps\",\n    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n    \"--save_total_limit\", \"3\",\n    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n    \"--adam_beta2\", \"0.95\",\n    \"--warmup_ratio\", \"0.1\",\n    \"--lr_scheduler_type\", \"cosine\",\n    \"--logging_steps\", \"10\",\n    \"--report_to\", \"none\",\n    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n    \"--lazy_preprocess\", \"False\",\n    \"--gradient_checkpointing\", \"True\",\n    \"--predict_with_generate\", \"True\",\n    \"--include_inputs_for_metrics\", \"True\",\n    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n    \"--torch_compile\", \"False\",\n    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n    \"--model_type\", \"auto\",\n    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n    \"--bf16\",\n    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n]\n\n# Show the full command for debugging\nprint(\"üìù Training command:\")\nprint(\" \".join(cmd))\nprint(\"\\n\" + \"=\"*60 + \"\\n\")\n\n# Run training\ntry:\n    result = subprocess.run(cmd, cwd=TRAINING_DIR, check=False)\n    \n    if result.returncode == 0:\n        print(\"\\n\" + \"=\"*60)\n        print(\"‚úÖ Training completed successfully!\")\n        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n        print(\"=\"*60)\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(f\"‚ùå Training failed with return code {result.returncode}\")\n        print(\"=\"*60)\n        print(\"\\nüí° Common issues:\")\n        print(\"  - Dataset not found: Check that nl2sql_standard_train_en.json exists\")\n        print(\"  - Model not found: Check MODEL_PATH is correct\")\n        print(\"  - Out of memory: Try reducing max_length or batch_size\")\n        print(\"\\nüîç Check the error messages above for more details\")\nexcept Exception as e:\n    print(f\"\\n‚ùå Error during training: {e}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "config_training"
   },
   "source": [
    "## Step 8: Save Trained Model to Google Drive (Optional)\n",
    "\n",
    "After training completes, save your model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8"
   },
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to trained model\n",
    "TRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Destination in Google Drive\n",
    "DRIVE_SAVE_PATH = f\"/content/drive/MyDrive/XiYan-SQL/Trained-Models/{TRAINING_CONFIG['expr_id']}\"\n",
    "\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    print(f\"üì• Copying trained model to Google Drive...\")\n",
    "    print(f\"   From: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   To: {DRIVE_SAVE_PATH}\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    # Copy model\n",
    "    if os.path.exists(DRIVE_SAVE_PATH):\n",
    "        shutil.rmtree(DRIVE_SAVE_PATH)\n",
    "    \n",
    "    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n",
    "    print(f\"\\n‚úÖ Model saved to Google Drive!\")\n",
    "    print(f\"üìÅ Location: {DRIVE_SAVE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n",
    "    print(\"Make sure training completed successfully in Step 7.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Set training directory\n",
    "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
    "os.chdir(TRAINING_DIR)\n",
    "\n",
    "# Create DeepSpeed config for single GPU (Colab typically has 1 GPU)\n",
    "ds_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"gradient_accumulation_steps\": TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"offload_optimizer_device\": \"cpu\",  # Offload to CPU to save GPU memory\n",
    "        \"offload_param_device\": \"cpu\",\n",
    "        \"zero3_init_flag\": False,\n",
    "        \"zero3_save_16bit_model\": False,\n",
    "        \"zero_stage\": 2,  # Use Zero2 for efficiency\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"machine_rank\": 0,\n",
    "    \"main_process_ip\": None,\n",
    "    \"main_process_port\": None,\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 1,  # Single GPU in Colab\n",
    "    \"rdzv_backend\": \"static\",\n",
    "    \"same_network\": True,\n",
    "    \"tpu_env\": [],\n",
    "    \"tpu_use_cluster\": False,\n",
    "    \"tpu_use_sudo\": False,\n",
    "    \"use_cpu\": False\n",
    "}\n",
    "\n",
    "# Save DeepSpeed config\n",
    "os.makedirs(\"train/config\", exist_ok=True)\n",
    "ds_config_path = \"train/config/colab_zero2.json\"\n",
    "with open(ds_config_path, 'w') as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\n",
    "print(f\"üìä Dataset: {TRAINING_CONFIG['data_path']}\")\n",
    "print(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\n",
    "print(\"\\n‚è≥ This may take several hours depending on dataset size...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Build training command\n",
    "cmd = [\n",
    "    \"accelerate\", \"launch\",\n",
    "    \"--config_file\", ds_config_path,\n",
    "    \"--num_processes\", \"1\",\n",
    "    \"train/sft4xiyan.py\",\n",
    "    \"--save_only_model\", \"True\",\n",
    "    \"--resume\", \"False\",\n",
    "    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n",
    "    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n",
    "    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n",
    "    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n",
    "    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
    "    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n",
    "    \"--save_total_limit\", \"3\",  # Keep only last 3 checkpoints\n",
    "    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
    "    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
    "    \"--adam_beta2\", \"0.95\",\n",
    "    \"--warmup_ratio\", \"0.1\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--report_to\", \"none\",\n",
    "    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n",
    "    \"--lazy_preprocess\", \"False\",\n",
    "    \"--gradient_checkpointing\", \"True\",\n",
    "    \"--predict_with_generate\", \"True\",\n",
    "    \"--include_inputs_for_metrics\", \"True\",\n",
    "    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n",
    "    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n",
    "    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n",
    "    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n",
    "    \"--torch_compile\", \"False\",\n",
    "    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n",
    "    \"--model_type\", \"auto\",\n",
    "    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n",
    "    \"--bf16\",\n",
    "    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n",
    "]\n",
    "\n",
    "# Run training\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=TRAINING_DIR,\n",
    "        check=False  # Don't raise on error, we'll check return code\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"‚ùå Training failed with return code {result.returncode}\")\n",
    "        print(\"\\nCommon issues:\")\n",
    "        print(\"  - Out of Memory (OOM): Reduce batch_size or max_length\")\n",
    "        print(\"  - Model not found: Check MODEL_PATH in Step 4\")\n",
    "        print(\"  - Dataset not found: Check data_path in Step 6\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9"
   },
   "source": [
    "## Step 9: Save Trained Model to Google Drive (Optional)\n",
    "\n",
    "After training completes, save your model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to trained model\n",
    "TRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Destination in Google Drive\n",
    "# ‚ö†Ô∏è UPDATE THIS: Where you want to save the trained model\n",
    "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/trained_models/nl2sql_3b_colab\"  # ‚ö†Ô∏è UPDATE THIS\n",
    "\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    print(f\"üì• Copying trained model to Google Drive...\")\n",
    "    print(f\"   From: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   To: {DRIVE_SAVE_PATH}\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    # Copy model\n",
    "    if os.path.exists(DRIVE_SAVE_PATH):\n",
    "        shutil.rmtree(DRIVE_SAVE_PATH)\n",
    "    \n",
    "    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n",
    "    print(f\"\\n‚úÖ Model saved to Google Drive: {DRIVE_SAVE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n",
    "    print(\"Make sure training completed successfully in Step 8.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Errors\n",
    "- Reduce `batch_size` to 1\n",
    "- Reduce `max_length` to 8192 or 4096\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- The DeepSpeed config already uses CPU offloading, which helps\n",
    "\n",
    "### Model Not Found\n",
    "- Check that `MODEL_DRIVE_PATH` in Step 4 is correct\n",
    "- Verify the model folder exists in Google Drive\n",
    "- Ensure the model folder contains all required files (config.json, tokenizer files, etc.)\n",
    "\n",
    "### Dataset Not Found\n",
    "- Check that dataset paths in Step 5 are correct\n",
    "- Verify files exist in Google Drive\n",
    "- If processing raw data, ensure `db_conn.json` exists\n",
    "\n",
    "### Training Too Slow\n",
    "- Colab free tier has limited GPU time\n",
    "- Consider using Colab Pro for longer training sessions\n",
    "- Reduce dataset size for testing (set `sample_num` in dataset config)\n",
    "\n",
    "### Connection Issues\n",
    "- Colab sessions may disconnect after inactivity\n",
    "- Use `nohup` or save checkpoints frequently\n",
    "- Consider running training in multiple sessions if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}