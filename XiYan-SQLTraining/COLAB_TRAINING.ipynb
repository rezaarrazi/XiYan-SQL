{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# XiYan-SQL Training on Google Colab\n",
    "\n",
    "This notebook provides a complete step-by-step guide to train the XiYan-SQL model on Google Colab.\n",
    "\n",
    "## Prerequisites\n",
    "- Upload your model files to Google Drive (e.g., `Qwen2.5-Coder-3B-Instruct` folder)\n",
    "- Upload your dataset files to Google Drive (raw data, processed data, or both)\n",
    "- Enable GPU runtime in Colab (Runtime ‚Üí Change runtime type ‚Üí GPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step1"
   },
   "source": [
    "## Step 1: Install Dependencies\n",
    "\n",
    "Install all required packages for XiYan-SQL training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install system dependencies\n",
    "!apt-get update -qq\n",
    "!apt-get install -y -qq libaio-dev  # Required for DeepSpeed\n",
    "\n",
    "# Install Python packages\n",
    "!pip install -q accelerate>=1.12.0\n",
    "!pip install -q datasets>=3.0.0\n",
    "!pip install -q deepspeed>=0.18.4\n",
    "!pip install -q llama-index>=0.9.6.post2\n",
    "!pip install -q markupsafe==2.1.3  # Pin to <3.0\n",
    "!pip install -q modelscope>=1.33.0\n",
    "!pip install -q mysql-connector-python>=9.5.0\n",
    "!pip install -q ninja>=1.13.0\n",
    "!pip install -q \"numpy>=1.23.0,<2.0\"\n",
    "!pip install -q packaging>=24.1\n",
    "!pip install -q pandas>=2.3.3\n",
    "!pip install -q peft==0.11.1\n",
    "!pip install -q \"protobuf>=6.33.3\"\n",
    "!pip install -q psycopg2-binary>=2.9.11\n",
    "!pip install -q sentencepiece>=0.2.1\n",
    "!pip install -q setuptools>=70.2.0\n",
    "!pip install -q sqlalchemy>=2.0.45\n",
    "!pip install -q sqlglot>=28.5.0\n",
    "!pip install -q swanlab>=0.7.6\n",
    "!pip install -q textdistance>=4.6.3\n",
    "!pip install -q \"torch==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -q \"torchaudio==2.9.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -q \"torchvision==0.24.0\" --index-url https://download.pytorch.org/whl/cu126\n",
    "!pip install -q transformers==4.42.3\n",
    "!pip install -q wheel>=0.45.1\n",
    "\n",
    "# Install flash-attn (optional, for faster attention)\n",
    "# Note: This may take a while to compile\n",
    "try:\n",
    "    !pip install -q flash-attn --no-build-isolation\n",
    "    print(\"‚úÖ flash-attn installed successfully\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  flash-attn installation failed, continuing without it\")\n",
    "\n",
    "print(\"\\n‚úÖ All dependencies installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step2"
   },
   "source": [
    "## Step 2: Clone Repository\n",
    "\n",
    "Clone the XiYan-SQL repository to Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Change to content directory\n",
    "import os\n",
    "import sys\n",
    "os.chdir('/content')\n",
    "\n",
    "# Clone the repository\n",
    "# Replace with your repository URL\n",
    "REPO_URL = \"https://github.com/rezaarrazi/XiYan-SQL.git\"  # ‚ö†Ô∏è UPDATE THIS\n",
    "\n",
    "if not os.path.exists('XiYan-SQL'):\n",
    "    os.system(f'git clone {REPO_URL}')\n",
    "    print(\"‚úÖ Repository cloned successfully\")\n",
    "else:\n",
    "    print(\"‚úÖ Repository already exists\")\n",
    "\n",
    "# Navigate to training directory\n",
    "os.chdir('XiYan-SQL/XiYan-SQLTraining')\n",
    "\n",
    "# Add to Python path so imports work correctly\n",
    "TRAINING_DIR = os.getcwd()\n",
    "if TRAINING_DIR not in sys.path:\n",
    "    sys.path.insert(0, TRAINING_DIR)\n",
    "if os.path.dirname(TRAINING_DIR) not in sys.path:\n",
    "    sys.path.insert(0, os.path.dirname(TRAINING_DIR))\n",
    "\n",
    "print(f\"\\nüìÅ Current directory: {os.getcwd()}\")\n",
    "print(f\"‚úÖ Python path configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step3"
   },
   "source": [
    "## Step 3: Mount Google Drive\n",
    "\n",
    "Mount your Google Drive to access model and dataset files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mount_drive"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "print(\"‚úÖ Google Drive mounted successfully\")\n",
    "print(\"\\nüìÇ Drive path: /content/drive/MyDrive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step4"
   },
   "source": [
    "## Step 4: Copy Model from Google Drive\n",
    "\n",
    "Copy your pre-downloaded model from Google Drive to the model directory.\n",
    "\n",
    "**Configured Path:** `My Drive/Xiyan-SQL/Models/Qwen/`\n",
    "\n",
    "The script will automatically detect and copy the model folder(s) from this location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_model"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to your model in Google Drive\n",
    "MODEL_DRIVE_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Models/Qwen\"\n",
    "\n",
    "# Target directory in the repository\n",
    "MODEL_TARGET_DIR = \"train/model/Qwen\"\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(MODEL_TARGET_DIR, exist_ok=True)\n",
    "\n",
    "# Check if model directory exists in Drive\n",
    "if os.path.exists(MODEL_DRIVE_PATH):\n",
    "    print(f\"üì• Found model directory at {MODEL_DRIVE_PATH}\")\n",
    "    \n",
    "    # List contents to see what's inside\n",
    "    contents = os.listdir(MODEL_DRIVE_PATH)\n",
    "    print(f\"üìÅ Contents: {contents}\")\n",
    "    \n",
    "    # Check if it's a single model folder or contains multiple model folders\n",
    "    model_folders = [item for item in contents if os.path.isdir(os.path.join(MODEL_DRIVE_PATH, item))]\n",
    "    \n",
    "    if len(model_folders) == 1:\n",
    "        # Single model folder - copy it directly\n",
    "        model_name = model_folders[0]\n",
    "        source_path = os.path.join(MODEL_DRIVE_PATH, model_name)\n",
    "        target_path = os.path.join(MODEL_TARGET_DIR, model_name)\n",
    "        \n",
    "        if os.path.exists(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying model '{model_name}' from {source_path}...\")\n",
    "            shutil.copytree(source_path, target_path)\n",
    "            print(f\"‚úÖ Model copied to {target_path}\")\n",
    "        \n",
    "        MODEL_PATH = target_path\n",
    "    else:\n",
    "        # Multiple folders or files - copy the entire Qwen directory\n",
    "        target_path = MODEL_TARGET_DIR\n",
    "        if os.path.exists(target_path) and os.listdir(target_path):\n",
    "            print(f\"‚ö†Ô∏è  Model directory already exists at {target_path}\")\n",
    "            print(\"Skipping copy (delete manually if you want to re-copy)\")\n",
    "        else:\n",
    "            print(f\"üì• Copying all models from {MODEL_DRIVE_PATH}...\")\n",
    "            for item in contents:\n",
    "                source_item = os.path.join(MODEL_DRIVE_PATH, item)\n",
    "                target_item = os.path.join(target_path, item)\n",
    "                if os.path.isdir(source_item):\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copytree(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "                else:\n",
    "                    if not os.path.exists(target_item):\n",
    "                        shutil.copy2(source_item, target_item)\n",
    "                        print(f\"  ‚úÖ Copied {item}\")\n",
    "            print(f\"‚úÖ All models copied to {target_path}\")\n",
    "        \n",
    "        # Set MODEL_PATH to the first model folder found, or let user specify\n",
    "        if model_folders:\n",
    "            MODEL_PATH = os.path.join(MODEL_TARGET_DIR, model_folders[0])\n",
    "            print(f\"\\nüìå Using model: {MODEL_PATH}\")\n",
    "            print(f\"üí° If you want to use a different model, update MODEL_PATH in Step 7\")\n",
    "        else:\n",
    "            MODEL_PATH = MODEL_TARGET_DIR\n",
    "            print(f\"\\nüìå Model directory: {MODEL_PATH}\")\n",
    "            print(f\"üí° Please specify the exact model folder name in Step 7\")\n",
    "    \n",
    "    print(f\"\\nüìå Model path for training: {MODEL_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ùå Model not found at {MODEL_DRIVE_PATH}\")\n",
    "    print(\"\\nPlease check:\")\n",
    "    print(\"1. Google Drive is mounted correctly\")\n",
    "    print(\"2. The path 'My Drive/Xiyan-SQL/Models/Qwen/' exists in your Drive\")\n",
    "    MODEL_PATH = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step5"
   },
   "source": "## Step 5: Verify Training Dataset\n\nThe English training dataset should already be in the repository (via Git LFS).\n\n**Expected file:** `train/datasets/nl2sql_standard_train_en.json` (55MB)\n\nIf the file is not present, you can download it from Google Drive as a backup."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "copy_dataset"
   },
   "outputs": [],
   "source": "import os\nimport json\n\n# Check if training dataset exists in repository\nTRAIN_DATASET_PATH = \"train/datasets/nl2sql_standard_train_en.json\"\n\nif os.path.exists(TRAIN_DATASET_PATH):\n    print(f\"‚úÖ Training dataset found in repository!\")\n    print(f\"   Path: {TRAIN_DATASET_PATH}\")\n    \n    size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n    print(f\"   Size: {size_mb:.1f} MB\")\n    \n    # Quick verification\n    with open(TRAIN_DATASET_PATH, 'r') as f:\n        data = json.load(f)\n        print(f\"   Samples: {len(data)}\")\n        \n        # Check if English\n        if data and data[0].get('conversations'):\n            prompt = data[0]['conversations'][0]['content']\n            if prompt.startswith(\"You are a SQLite expert\"):\n                print(f\"   Language: ‚úÖ English\")\n            else:\n                print(f\"   Language: ‚ö†Ô∏è Not English\")\n    \n    print(\"\\nüéâ Ready to start training! Skip to Step 6.\")\n    \nelse:\n    print(f\"‚ö†Ô∏è  Training dataset not found in repository\")\n    print(f\"   Expected: {TRAIN_DATASET_PATH}\")\n    print(\"\\nüì• Attempting to download from Google Drive as backup...\")\n    \n    # Backup: Download from Google Drive\n    DRIVE_DATASET_PATH = \"/content/drive/MyDrive/Xiyan-SQL/Dataset/nl2sql_standard_train_en.json\"\n    \n    if os.path.exists(DRIVE_DATASET_PATH):\n        import shutil\n        os.makedirs(\"train/datasets\", exist_ok=True)\n        shutil.copy2(DRIVE_DATASET_PATH, TRAIN_DATASET_PATH)\n        print(f\"‚úÖ Dataset copied from Google Drive\")\n        \n        size_mb = os.path.getsize(TRAIN_DATASET_PATH) / (1024 * 1024)\n        print(f\"   Size: {size_mb:.1f} MB\")\n    else:\n        print(f\"‚ùå Dataset not found in Google Drive either\")\n        print(f\"   Expected: {DRIVE_DATASET_PATH}\")\n        print(\"\\nüí° Options:\")\n        print(\"1. Make sure Git LFS pulled the dataset when cloning\")\n        print(\"2. Upload nl2sql_standard_train_en.json to Google Drive\")\n        print(\"3. Or run data processing from BIRD raw data\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step6"
   },
   "source": "## Step 6: Configure Training Parameters\n\nOptimized for Google Colab with 15GB GPU (T4 or better).\n\nAdjust these parameters based on your GPU memory and requirements."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "process_data"
   },
   "outputs": [],
   "source": "# Check available GPU memory\nimport subprocess\nimport re\n\ntry:\n    result = subprocess.run(['nvidia-smi', '--query-gpu=memory.total', '--format=csv,noheader,nounits'], \n                          capture_output=True, text=True)\n    gpu_memory_mb = int(result.stdout.strip())\n    gpu_memory_gb = gpu_memory_mb / 1024\n    print(f\"üéÆ Detected GPU Memory: {gpu_memory_gb:.1f} GB\")\nexcept:\n    gpu_memory_gb = 15.0  # Default assumption\n    print(f\"‚ö†Ô∏è  Could not detect GPU, assuming {gpu_memory_gb} GB\")\n\n# Auto-configure based on GPU memory\nif gpu_memory_gb >= 14:\n    # 15GB GPU (T4, L4, etc.) - Optimized settings\n    MAX_LENGTH = 8192\n    LORA_R = 128\n    BATCH_SIZE = 2\n    GRAD_ACC = 16\n    print(f\"üìä Using HIGH MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\nelif gpu_memory_gb >= 10:\n    # 12GB GPU (T4 with less memory) - Moderate settings\n    MAX_LENGTH = 4096\n    LORA_R = 64\n    BATCH_SIZE = 1\n    GRAD_ACC = 32\n    print(f\"üìä Using MEDIUM MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\nelse:\n    # 8GB GPU (limited memory) - Conservative settings\n    MAX_LENGTH = 2048\n    LORA_R = 32\n    BATCH_SIZE = 1\n    GRAD_ACC = 64\n    print(f\"üìä Using LOW MEMORY config for {gpu_memory_gb:.1f}GB GPU\")\n\nTRAINING_CONFIG = {\n    # Experiment ID\n    \"expr_id\": \"nl2sql_3b_colab_en\",\n    \n    # Model path (set in Step 4)\n    \"model_path\": MODEL_PATH if 'MODEL_PATH' in globals() else \"train/model/Qwen/Qwen2.5-Coder-3B-Instruct\",\n    \n    # Dataset path - Using English version\n    \"data_path\": \"train/datasets/nl2sql_standard_train_en.json\",\n    \n    # Output directory\n    \"output_dir\": \"train/output/dense/nl2sql_3b_colab_en/\",\n    \n    # Training hyperparameters\n    \"epochs\": 3,  # Reduced for faster training in Colab\n    \"learning_rate\": 2e-5,\n    \"weight_decay\": 0.1,\n    \"max_length\": MAX_LENGTH,\n    \n    # LoRA configuration\n    \"use_lora\": True,\n    \"lora_r\": LORA_R,\n    \"lora_alpha\": LORA_R * 2,\n    \n    # Batch configuration\n    \"batch_size\": BATCH_SIZE,\n    \"gradient_accumulation_steps\": GRAD_ACC,\n    \n    # Other settings\n    \"save_steps\": 200,\n    \"group_by_length\": True,\n    \"shuffle\": True,\n    \"use_flash_attention\": True,\n    \"bf16\": True,\n}\n\nprint(\"\\nüìã Training Configuration:\")\nprint(f\"  Experiment ID: {TRAINING_CONFIG['expr_id']}\")\nprint(f\"  Dataset: {TRAINING_CONFIG['data_path']}\")\nprint(f\"  Max Length: {TRAINING_CONFIG['max_length']} tokens\")\nprint(f\"  LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\nprint(f\"  Batch Size: {TRAINING_CONFIG['batch_size']}\")\nprint(f\"  Gradient Accumulation: {TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"  Effective Batch Size: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"  Epochs: {TRAINING_CONFIG['epochs']}\")\nprint(f\"  Learning Rate: {TRAINING_CONFIG['learning_rate']}\")\n\nprint(\"\\nüí° Estimated Training Time:\")\nsamples = 9431\nsteps_per_epoch = samples // (TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps'])\ntotal_steps = steps_per_epoch * TRAINING_CONFIG['epochs']\ntime_per_step_sec = 3  # Conservative estimate\ntotal_hours = (total_steps * time_per_step_sec) / 3600\nprint(f\"  Steps per epoch: ~{steps_per_epoch}\")\nprint(f\"  Total steps: ~{total_steps}\")\nprint(f\"  Estimated time: ~{total_hours:.1f} hours\")\n\nprint(\"\\n‚ö†Ô∏è  Colab Tips:\")\nprint(\"  - Free tier: 12 hour runtime limit\")\nprint(\"  - Keep browser tab active to prevent disconnection\")\nprint(\"  - Consider Colab Pro for longer sessions\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "assemble_data"
   },
   "outputs": [],
   "source": "## Step 7: Start Training\n\nRun the training with your optimized configuration."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step7"
   },
   "source": "import os\nimport subprocess\nimport json\n\n# Set training directory\nTRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\nos.chdir(TRAINING_DIR)\n\n# Create DeepSpeed config for single GPU\nds_config = {\n    \"compute_environment\": \"LOCAL_MACHINE\",\n    \"distributed_type\": \"DEEPSPEED\",\n    \"deepspeed_config\": {\n        \"gradient_accumulation_steps\": TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n        \"gradient_clipping\": 1.0,\n        \"offload_optimizer_device\": \"cpu\",\n        \"offload_param_device\": \"cpu\",\n        \"zero3_init_flag\": False,\n        \"zero3_save_16bit_model\": False,\n        \"zero_stage\": 2,\n        \"bf16\": {\n            \"enabled\": True\n        }\n    },\n    \"machine_rank\": 0,\n    \"main_process_ip\": None,\n    \"main_process_port\": None,\n    \"num_machines\": 1,\n    \"num_processes\": 1,\n    \"rdzv_backend\": \"static\",\n    \"same_network\": True,\n    \"tpu_env\": [],\n    \"tpu_use_cluster\": False,\n    \"tpu_use_sudo\": False,\n    \"use_cpu\": False\n}\n\n# Save DeepSpeed config\nos.makedirs(\"train/config\", exist_ok=True)\nds_config_path = \"train/config/colab_zero2.json\"\nwith open(ds_config_path, 'w') as f:\n    json.dump(ds_config, f, indent=2)\n\nprint(\"üöÄ Starting XiYan-SQL Training\")\nprint(\"=\"*60)\nprint(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\nprint(f\"üìä Dataset: {TRAINING_CONFIG['data_path']} (English)\")\nprint(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\nprint(f\"üéØ Effective Batch: {TRAINING_CONFIG['batch_size'] * TRAINING_CONFIG['gradient_accumulation_steps']}\")\nprint(f\"üìè Max Length: {TRAINING_CONFIG['max_length']} tokens\")\nprint(f\"üîß LoRA Rank: {TRAINING_CONFIG['lora_r']}\")\nprint(\"=\"*60)\nprint(\"\\n‚è≥ Training will take several hours...\")\nprint(\"üí° Keep this tab active to prevent disconnection\\n\")\n\n# Build training command\ncmd = [\n    \"accelerate\", \"launch\",\n    \"--config_file\", ds_config_path,\n    \"--num_processes\", \"1\",\n    \"train/sft4xiyan.py\",\n    \"--save_only_model\", \"True\",\n    \"--resume\", \"False\",\n    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n    \"--save_strategy\", \"steps\",\n    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n    \"--save_total_limit\", \"3\",\n    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n    \"--adam_beta2\", \"0.95\",\n    \"--warmup_ratio\", \"0.1\",\n    \"--lr_scheduler_type\", \"cosine\",\n    \"--logging_steps\", \"10\",\n    \"--report_to\", \"none\",\n    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n    \"--lazy_preprocess\", \"False\",\n    \"--gradient_checkpointing\", \"True\",\n    \"--predict_with_generate\", \"True\",\n    \"--include_inputs_for_metrics\", \"True\",\n    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n    \"--torch_compile\", \"False\",\n    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n    \"--model_type\", \"auto\",\n    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n    \"--bf16\",\n    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n]\n\n# Run training\ntry:\n    result = subprocess.run(cmd, cwd=TRAINING_DIR, check=False)\n    \n    if result.returncode == 0:\n        print(\"\\n\" + \"=\"*60)\n        print(\"‚úÖ Training completed successfully!\")\n        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n        print(\"=\"*60)\n    else:\n        print(\"\\n\" + \"=\"*60)\n        print(f\"‚ùå Training failed with return code {result.returncode}\")\n        print(\"=\"*60)\nexcept Exception as e:\n    print(f\"\\n‚ùå Error during training: {e}\")"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "config_training"
   },
   "outputs": [],
   "source": "## Step 8: Save Trained Model to Google Drive (Optional)\n\nAfter training completes, save your model to Google Drive for future use."
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step8"
   },
   "source": "import shutil\nimport os\n\n# Path to trained model\nTRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n\n# Destination in Google Drive\nDRIVE_SAVE_PATH = f\"/content/drive/MyDrive/XiYan-SQL/Trained-Models/{TRAINING_CONFIG['expr_id']}\"\n\nif os.path.exists(TRAINED_MODEL_PATH):\n    print(f\"üì• Copying trained model to Google Drive...\")\n    print(f\"   From: {TRAINED_MODEL_PATH}\")\n    print(f\"   To: {DRIVE_SAVE_PATH}\")\n    \n    # Create parent directory\n    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n    \n    # Copy model\n    if os.path.exists(DRIVE_SAVE_PATH):\n        shutil.rmtree(DRIVE_SAVE_PATH)\n    \n    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n    print(f\"\\n‚úÖ Model saved to Google Drive!\")\n    print(f\"üìÅ Location: {DRIVE_SAVE_PATH}\")\nelse:\n    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n    print(\"Make sure training completed successfully in Step 7.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "train_model"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "# Set training directory\n",
    "TRAINING_DIR = \"/content/XiYan-SQL/XiYan-SQLTraining\"\n",
    "os.chdir(TRAINING_DIR)\n",
    "\n",
    "# Create DeepSpeed config for single GPU (Colab typically has 1 GPU)\n",
    "ds_config = {\n",
    "    \"compute_environment\": \"LOCAL_MACHINE\",\n",
    "    \"distributed_type\": \"DEEPSPEED\",\n",
    "    \"deepspeed_config\": {\n",
    "        \"gradient_accumulation_steps\": TRAINING_CONFIG[\"gradient_accumulation_steps\"],\n",
    "        \"gradient_clipping\": 1.0,\n",
    "        \"offload_optimizer_device\": \"cpu\",  # Offload to CPU to save GPU memory\n",
    "        \"offload_param_device\": \"cpu\",\n",
    "        \"zero3_init_flag\": False,\n",
    "        \"zero3_save_16bit_model\": False,\n",
    "        \"zero_stage\": 2,  # Use Zero2 for efficiency\n",
    "        \"bf16\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"machine_rank\": 0,\n",
    "    \"main_process_ip\": None,\n",
    "    \"main_process_port\": None,\n",
    "    \"num_machines\": 1,\n",
    "    \"num_processes\": 1,  # Single GPU in Colab\n",
    "    \"rdzv_backend\": \"static\",\n",
    "    \"same_network\": True,\n",
    "    \"tpu_env\": [],\n",
    "    \"tpu_use_cluster\": False,\n",
    "    \"tpu_use_sudo\": False,\n",
    "    \"use_cpu\": False\n",
    "}\n",
    "\n",
    "# Save DeepSpeed config\n",
    "os.makedirs(\"train/config\", exist_ok=True)\n",
    "ds_config_path = \"train/config/colab_zero2.json\"\n",
    "with open(ds_config_path, 'w') as f:\n",
    "    json.dump(ds_config, f, indent=2)\n",
    "\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(f\"üìÅ Model: {TRAINING_CONFIG['model_path']}\")\n",
    "print(f\"üìä Dataset: {TRAINING_CONFIG['data_path']}\")\n",
    "print(f\"üíæ Output: {TRAINING_CONFIG['output_dir']}\")\n",
    "print(\"\\n‚è≥ This may take several hours depending on dataset size...\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Build training command\n",
    "cmd = [\n",
    "    \"accelerate\", \"launch\",\n",
    "    \"--config_file\", ds_config_path,\n",
    "    \"--num_processes\", \"1\",\n",
    "    \"train/sft4xiyan.py\",\n",
    "    \"--save_only_model\", \"True\",\n",
    "    \"--resume\", \"False\",\n",
    "    \"--model_name_or_path\", TRAINING_CONFIG[\"model_path\"],\n",
    "    \"--data_path\", TRAINING_CONFIG[\"data_path\"],\n",
    "    \"--output_dir\", TRAINING_CONFIG[\"output_dir\"],\n",
    "    \"--num_train_epochs\", str(TRAINING_CONFIG[\"epochs\"]),\n",
    "    \"--per_device_train_batch_size\", str(TRAINING_CONFIG[\"batch_size\"]),\n",
    "    \"--gradient_accumulation_steps\", str(TRAINING_CONFIG[\"gradient_accumulation_steps\"]),\n",
    "    \"--save_strategy\", \"steps\",\n",
    "    \"--save_steps\", str(TRAINING_CONFIG[\"save_steps\"]),\n",
    "    \"--save_total_limit\", \"3\",  # Keep only last 3 checkpoints\n",
    "    \"--learning_rate\", str(TRAINING_CONFIG[\"learning_rate\"]),\n",
    "    \"--weight_decay\", str(TRAINING_CONFIG[\"weight_decay\"]),\n",
    "    \"--adam_beta2\", \"0.95\",\n",
    "    \"--warmup_ratio\", \"0.1\",\n",
    "    \"--lr_scheduler_type\", \"cosine\",\n",
    "    \"--logging_steps\", \"10\",\n",
    "    \"--report_to\", \"none\",\n",
    "    \"--model_max_length\", str(TRAINING_CONFIG[\"max_length\"]),\n",
    "    \"--lazy_preprocess\", \"False\",\n",
    "    \"--gradient_checkpointing\", \"True\",\n",
    "    \"--predict_with_generate\", \"True\",\n",
    "    \"--include_inputs_for_metrics\", \"True\",\n",
    "    \"--use_lora\", str(TRAINING_CONFIG[\"use_lora\"]),\n",
    "    \"--lora_r\", str(TRAINING_CONFIG[\"lora_r\"]),\n",
    "    \"--lora_alpha\", str(TRAINING_CONFIG[\"lora_alpha\"]),\n",
    "    \"--do_shuffle\", str(TRAINING_CONFIG[\"shuffle\"]),\n",
    "    \"--torch_compile\", \"False\",\n",
    "    \"--group_by_length\", str(TRAINING_CONFIG[\"group_by_length\"]),\n",
    "    \"--model_type\", \"auto\",\n",
    "    \"--use_flash_attention\", str(TRAINING_CONFIG[\"use_flash_attention\"]),\n",
    "    \"--bf16\",\n",
    "    \"--expr_id\", TRAINING_CONFIG[\"expr_id\"]\n",
    "]\n",
    "\n",
    "# Run training\n",
    "try:\n",
    "    result = subprocess.run(\n",
    "        cmd,\n",
    "        cwd=TRAINING_DIR,\n",
    "        check=False  # Don't raise on error, we'll check return code\n",
    "    )\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"‚úÖ Training completed successfully!\")\n",
    "        print(f\"üìÅ Model saved to: {TRAINING_CONFIG['output_dir']}\")\n",
    "    else:\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(f\"‚ùå Training failed with return code {result.returncode}\")\n",
    "        print(\"\\nCommon issues:\")\n",
    "        print(\"  - Out of Memory (OOM): Reduce batch_size or max_length\")\n",
    "        print(\"  - Model not found: Check MODEL_PATH in Step 4\")\n",
    "        print(\"  - Dataset not found: Check data_path in Step 6\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error during training: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "step9"
   },
   "source": [
    "## Step 9: Save Trained Model to Google Drive (Optional)\n",
    "\n",
    "After training completes, save your model to Google Drive for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "save_to_drive"
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "import os\n",
    "\n",
    "# Path to trained model\n",
    "TRAINED_MODEL_PATH = TRAINING_CONFIG[\"output_dir\"]\n",
    "\n",
    "# Destination in Google Drive\n",
    "# ‚ö†Ô∏è UPDATE THIS: Where you want to save the trained model\n",
    "DRIVE_SAVE_PATH = \"/content/drive/MyDrive/trained_models/nl2sql_3b_colab\"  # ‚ö†Ô∏è UPDATE THIS\n",
    "\n",
    "if os.path.exists(TRAINED_MODEL_PATH):\n",
    "    print(f\"üì• Copying trained model to Google Drive...\")\n",
    "    print(f\"   From: {TRAINED_MODEL_PATH}\")\n",
    "    print(f\"   To: {DRIVE_SAVE_PATH}\")\n",
    "    \n",
    "    # Create parent directory\n",
    "    os.makedirs(os.path.dirname(DRIVE_SAVE_PATH), exist_ok=True)\n",
    "    \n",
    "    # Copy model\n",
    "    if os.path.exists(DRIVE_SAVE_PATH):\n",
    "        shutil.rmtree(DRIVE_SAVE_PATH)\n",
    "    \n",
    "    shutil.copytree(TRAINED_MODEL_PATH, DRIVE_SAVE_PATH)\n",
    "    print(f\"\\n‚úÖ Model saved to Google Drive: {DRIVE_SAVE_PATH}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Trained model not found at {TRAINED_MODEL_PATH}\")\n",
    "    print(\"Make sure training completed successfully in Step 8.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "troubleshooting"
   },
   "source": [
    "## Troubleshooting\n",
    "\n",
    "### Out of Memory (OOM) Errors\n",
    "- Reduce `batch_size` to 1\n",
    "- Reduce `max_length` to 8192 or 4096\n",
    "- Increase `gradient_accumulation_steps` to maintain effective batch size\n",
    "- The DeepSpeed config already uses CPU offloading, which helps\n",
    "\n",
    "### Model Not Found\n",
    "- Check that `MODEL_DRIVE_PATH` in Step 4 is correct\n",
    "- Verify the model folder exists in Google Drive\n",
    "- Ensure the model folder contains all required files (config.json, tokenizer files, etc.)\n",
    "\n",
    "### Dataset Not Found\n",
    "- Check that dataset paths in Step 5 are correct\n",
    "- Verify files exist in Google Drive\n",
    "- If processing raw data, ensure `db_conn.json` exists\n",
    "\n",
    "### Training Too Slow\n",
    "- Colab free tier has limited GPU time\n",
    "- Consider using Colab Pro for longer training sessions\n",
    "- Reduce dataset size for testing (set `sample_num` in dataset config)\n",
    "\n",
    "### Connection Issues\n",
    "- Colab sessions may disconnect after inactivity\n",
    "- Use `nohup` or save checkpoints frequently\n",
    "- Consider running training in multiple sessions if needed"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}